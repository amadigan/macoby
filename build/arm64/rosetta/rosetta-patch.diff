diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 100570a048c5..906237eb81c2 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -2060,200 +2060,252 @@ endmenu # "ARMv8.4 architectural features"
 menu "ARMv8.5 architectural features"
 
 config AS_HAS_ARMV8_5
 	def_bool $(cc-option,-Wa$(comma)-march=armv8.5-a)
 
 config ARM64_BTI
 	bool "Branch Target Identification support"
 	default y
 	help
 	  Branch Target Identification (part of the ARMv8.5 Extensions)
 	  provides a mechanism to limit the set of locations to which computed
 	  branch instructions such as BR or BLR can jump.
 
 	  To make use of BTI on CPUs that support it, say Y.
 
 	  BTI is intended to provide complementary protection to other control
 	  flow integrity protection mechanisms, such as the Pointer
 	  authentication mechanism provided as part of the ARMv8.3 Extensions.
 	  For this reason, it does not make sense to enable this option without
 	  also enabling support for pointer authentication.  Thus, when
 	  enabling this option you should also select ARM64_PTR_AUTH=y.
 
 	  Userspace binaries must also be specifically compiled to make use of
 	  this mechanism.  If you say N here or the hardware does not support
 	  BTI, such binaries can still run, but you get no additional
 	  enforcement of branch destinations.
 
 config ARM64_BTI_KERNEL
 	bool "Use Branch Target Identification for kernel"
 	default y
 	depends on ARM64_BTI
 	depends on ARM64_PTR_AUTH_KERNEL
 	depends on CC_HAS_BRANCH_PROT_PAC_RET_BTI
 	# https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94697
 	depends on !CC_IS_GCC || GCC_VERSION >= 100100
 	# https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106671
 	depends on !CC_IS_GCC
 	depends on (!FUNCTION_GRAPH_TRACER || DYNAMIC_FTRACE_WITH_ARGS)
 	help
 	  Build the kernel with Branch Target Identification annotations
 	  and enable enforcement of this for kernel code. When this option
 	  is enabled and the system supports BTI all kernel code including
 	  modular code must have BTI enabled.
 
 config CC_HAS_BRANCH_PROT_PAC_RET_BTI
 	# GCC 9 or later, clang 8 or later
 	def_bool $(cc-option,-mbranch-protection=pac-ret+leaf+bti)
 
 config ARM64_E0PD
 	bool "Enable support for E0PD"
 	default y
 	help
 	  E0PD (part of the ARMv8.5 extensions) allows us to ensure
 	  that EL0 accesses made via TTBR1 always fault in constant time,
 	  providing similar benefits to KASLR as those provided by KPTI, but
 	  with lower overhead and without disrupting legitimate access to
 	  kernel memory such as SPE.
 
 	  This option enables E0PD for TTBR1 where available.
 
 config ARM64_AS_HAS_MTE
 	# Initial support for MTE went in binutils 2.32.0, checked with
 	# ".arch armv8.5-a+memtag" below. However, this was incomplete
 	# as a late addition to the final architecture spec (LDGM/STGM)
 	# is only supported in the newer 2.32.x and 2.33 binutils
 	# versions, hence the extra "stgm" instruction check below.
 	def_bool $(as-instr,.arch armv8.5-a+memtag\nstgm xzr$(comma)[x0])
 
 config ARM64_MTE
 	bool "Memory Tagging Extension support"
 	default y
 	depends on ARM64_AS_HAS_MTE && ARM64_TAGGED_ADDR_ABI
 	depends on AS_HAS_ARMV8_5
 	depends on AS_HAS_LSE_ATOMICS
 	# Required for tag checking in the uaccess routines
 	depends on ARM64_PAN
 	select ARCH_HAS_SUBPAGE_FAULTS
 	select ARCH_USES_HIGH_VMA_FLAGS
 	select ARCH_USES_PG_ARCH_2
 	select ARCH_USES_PG_ARCH_3
 	help
 	  Memory Tagging (part of the ARMv8.5 Extensions) provides
 	  architectural support for run-time, always-on detection of
 	  various classes of memory error to aid with software debugging
 	  to eliminate vulnerabilities arising from memory-unsafe
 	  languages.
 
 	  This option enables the support for the Memory Tagging
 	  Extension at EL0 (i.e. for userspace).
 
 	  Selecting this option allows the feature to be detected at
 	  runtime. Any secondary CPU not implementing this feature will
 	  not be allowed a late bring-up.
 
 	  Userspace binaries that want to use this feature must
 	  explicitly opt in. The mechanism for the userspace is
 	  described in:
 
 	  Documentation/arch/arm64/memory-tagging-extension.rst.
 
+# Copyright © 2024 Apple Inc. All Rights Reserved.
+# Disclaimer: IMPORTANT: This Apple software is supplied to you by Apple Inc.
+# ("Apple") in consideration of your agreement to the following terms, and
+# your use, installation, modification or redistribution of this Apple
+# software constitutes acceptance of these terms. If you do not agree with
+# these terms, please do not use, install, modify or redistribute this Apple
+# software.
+# In consideration of your agreement to abide by the following terms, and
+# subject to these terms, Apple grants you a personal, non-exclusive license,
+# under Apple's copyrights in this original Apple software (the "Apple
+# Software"), to use, reproduce, modify and redistribute the Apple Software,
+# with or without modifications, in source and/or binary forms; provided that
+# if you redistribute the Apple Software in its entirety and without
+# modifications, you must retain this notice and the following text and
+# disclaimers in all such redistributions of the Apple Software. Neither the
+# name, trademarks, service marks or logos of Apple Inc. may be used to
+# endorse or promote products derived from the Apple Software without specific
+# prior written permission from Apple. Except as expressly stated in this
+# notice, no other rights or licenses, express or implied, are granted by
+# Apple herein, including but not limited to any patent rights that may be
+# infringed by your derivative works or by other works in which the Apple
+# Software may be incorporated.
+# The Apple Software is provided by Apple on an "AS IS" basis. APPLE MAKES NO
+# WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED
+# WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+# PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN
+# COMBINATION WITH YOUR PRODUCTS.
+# IN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR
+# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
+# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+# INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION
+# AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER
+# THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR
+# OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+config ARM64_TSO
+	bool "ARM64 Apple Silicon TSO support"
+	default y
+	help
+	  Apple Silicon TSO mode allows the CPU's memory model to be
+	  dynamically switched between the default ARM64 memory model
+	  and x86_64's memory model (TSO).
+
+	  This option enables the support for toggling TSO mode for
+	  userspace threads.
+
+	  Selecting this option allows the feature to be detected at
+	  runtime. If the CPU doesn't implement TSO mode, then this
+	  feature will be disabled.
+
+	  Userspace threads that want to use this feature must
+	  explicitly opt in via a prctl().
+
 endmenu # "ARMv8.5 architectural features"
 
 menu "ARMv8.7 architectural features"
 
 config ARM64_EPAN
 	bool "Enable support for Enhanced Privileged Access Never (EPAN)"
 	default y
 	depends on ARM64_PAN
 	help
 	  Enhanced Privileged Access Never (EPAN) allows Privileged
 	  Access Never to be used with Execute-only mappings.
 
 	  The feature is detected at runtime, and will remain disabled
 	  if the cpu does not implement the feature.
 endmenu # "ARMv8.7 architectural features"
 
 config AS_HAS_MOPS
 	def_bool $(as-instr,.arch_extension mops)
 
 menu "ARMv8.9 architectural features"
 
 config ARM64_POE
 	prompt "Permission Overlay Extension"
 	def_bool y
 	select ARCH_USES_HIGH_VMA_FLAGS
 	select ARCH_HAS_PKEYS
 	help
 	  The Permission Overlay Extension is used to implement Memory
 	  Protection Keys. Memory Protection Keys provides a mechanism for
 	  enforcing page-based protections, but without requiring modification
 	  of the page tables when an application changes protection domains.
 
 	  For details, see Documentation/core-api/protection-keys.rst
 
 	  If unsure, say y.
 
 config ARCH_PKEY_BITS
 	int
 	default 3
 
 config ARM64_HAFT
 	bool "Support for Hardware managed Access Flag for Table Descriptors"
 	depends on ARM64_HW_AFDBM
 	default y
 	help
 	  The ARMv8.9/ARMv9.5 introduces the feature Hardware managed Access
 	  Flag for Table descriptors. When enabled an architectural executed
 	  memory access will update the Access Flag in each Table descriptor
 	  which is accessed during the translation table walk and for which
 	  the Access Flag is 0. The Access Flag of the Table descriptor use
 	  the same bit of PTE_AF.
 
 	  The feature will only be enabled if all the CPUs in the system
 	  support this feature. If unsure, say Y.
 
 endmenu # "ARMv8.9 architectural features"
 
 menu "v9.4 architectural features"
 
 config ARM64_GCS
 	bool "Enable support for Guarded Control Stack (GCS)"
 	default y
 	select ARCH_HAS_USER_SHADOW_STACK
 	select ARCH_USES_HIGH_VMA_FLAGS
 	depends on !UPROBES
 	help
 	  Guarded Control Stack (GCS) provides support for a separate
 	  stack with restricted access which contains only return
 	  addresses.  This can be used to harden against some attacks
 	  by comparing return address used by the program with what is
 	  stored in the GCS, and may also be used to efficiently obtain
 	  the call stack for applications such as profiling.
 
 	  The feature is detected at runtime, and will remain disabled
 	  if the system does not implement the feature.
 
 endmenu # "v9.4 architectural features"
 
 config ARM64_SVE
 	bool "ARM Scalable Vector Extension support"
 	default y
 	help
 	  The Scalable Vector Extension (SVE) is an extension to the AArch64
 	  execution state which complements and extends the SIMD functionality
 	  of the base architecture to support much larger vectors and to enable
 	  additional vectorisation opportunities.
 
 	  To enable use of this extension on CPUs that implement it, say Y.
 
 	  On CPUs that support the SVE2 extensions, this option will enable
 	  those too.
 
 	  Note that for architectural reasons, firmware _must_ implement SVE
 	  support when running on SVE capable hardware.  The required support
 	  is present in:
 
 	    * version 1.5 and later of the ARM Trusted Firmware
 	    * the AArch64 boot wrapper since commit 5e1261e08abf
 	      ("bootwrapper: SVE: Enable SVE for EL2 and below").
 
diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 1bf1a3b16e88..30facbc3ad8e 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -95,200 +95,239 @@
 #define arch_get_mmap_end(addr, len, flags) \
 		(((addr) > DEFAULT_MAP_WINDOW) ? TASK_SIZE : DEFAULT_MAP_WINDOW)
 
 #define arch_get_mmap_base(addr, base) ((addr > DEFAULT_MAP_WINDOW) ? \
 					base + TASK_SIZE - DEFAULT_MAP_WINDOW :\
 					base)
 #endif /* CONFIG_ARM64_FORCE_52BIT */
 
 extern phys_addr_t arm64_dma_phys_limit;
 #define ARCH_LOW_ADDRESS_LIMIT	(arm64_dma_phys_limit - 1)
 
 struct debug_info {
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	/* Have we suspended stepping by a debugger? */
 	int			suspended_step;
 	/* Allow breakpoints and watchpoints to be disabled for this thread. */
 	int			bps_disabled;
 	int			wps_disabled;
 	/* Hardware breakpoints pinned to this task. */
 	struct perf_event	*hbp_break[ARM_MAX_BRP];
 	struct perf_event	*hbp_watch[ARM_MAX_WRP];
 #endif
 };
 
 enum vec_type {
 	ARM64_VEC_SVE = 0,
 	ARM64_VEC_SME,
 	ARM64_VEC_MAX,
 };
 
 enum fp_type {
 	FP_STATE_CURRENT,	/* Save based on current task state. */
 	FP_STATE_FPSIMD,
 	FP_STATE_SVE,
 };
 
 struct cpu_context {
 	unsigned long x19;
 	unsigned long x20;
 	unsigned long x21;
 	unsigned long x22;
 	unsigned long x23;
 	unsigned long x24;
 	unsigned long x25;
 	unsigned long x26;
 	unsigned long x27;
 	unsigned long x28;
 	unsigned long fp;
 	unsigned long sp;
 	unsigned long pc;
 };
 
 struct thread_struct {
 	struct cpu_context	cpu_context;	/* cpu context */
 
 	/*
 	 * Whitelisted fields for hardened usercopy:
 	 * Maintainers must ensure manually that this contains no
 	 * implicit padding.
 	 */
 	struct {
 		unsigned long	tp_value;	/* TLS register */
 		unsigned long	tp2_value;
 		u64		fpmr;
 		unsigned long	pad;
 		struct user_fpsimd_state fpsimd_state;
 	} uw;
 
 	enum fp_type		fp_type;	/* registers FPSIMD or SVE? */
 	unsigned int		fpsimd_cpu;
 	void			*sve_state;	/* SVE registers, if any */
 	void			*sme_state;	/* ZA and ZT state, if any */
 	unsigned int		vl[ARM64_VEC_MAX];	/* vector length */
 	unsigned int		vl_onexec[ARM64_VEC_MAX]; /* vl after next exec */
 	unsigned long		fault_address;	/* fault info */
 	unsigned long		fault_code;	/* ESR_EL1 value */
 	struct debug_info	debug;		/* debugging */
 
 	struct user_fpsimd_state	kernel_fpsimd_state;
 	unsigned int			kernel_fpsimd_cpu;
 #ifdef CONFIG_ARM64_PTR_AUTH
 	struct ptrauth_keys_user	keys_user;
 #ifdef CONFIG_ARM64_PTR_AUTH_KERNEL
 	struct ptrauth_keys_kernel	keys_kernel;
 #endif
 #endif
 #ifdef CONFIG_ARM64_MTE
 	u64			mte_ctrl;
 #endif
 	u64			sctlr_user;
 	u64			svcr;
 	u64			tpidr2_el0;
 	u64			por_el0;
 #ifdef CONFIG_ARM64_GCS
 	unsigned int		gcs_el0_mode;
 	unsigned int		gcs_el0_locked;
 	u64			gcspr_el0;
 	u64			gcs_base;
 	u64			gcs_size;
 #endif
+/*
+ * Copyright © 2024 Apple Inc. All Rights Reserved.
+ * Disclaimer: IMPORTANT: This Apple software is supplied to you by Apple Inc.
+ * ("Apple") in consideration of your agreement to the following terms, and
+ * your use, installation, modification or redistribution of this Apple
+ * software constitutes acceptance of these terms. If you do not agree with
+ * these terms, please do not use, install, modify or redistribute this Apple
+ * software.
+ * In consideration of your agreement to abide by the following terms, and
+ * subject to these terms, Apple grants you a personal, non-exclusive license,
+ * under Apple's copyrights in this original Apple software (the "Apple
+ * Software"), to use, reproduce, modify and redistribute the Apple Software,
+ * with or without modifications, in source and/or binary forms; provided that
+ * if you redistribute the Apple Software in its entirety and without
+ * modifications, you must retain this notice and the following text and
+ * disclaimers in all such redistributions of the Apple Software. Neither the
+ * name, trademarks, service marks or logos of Apple Inc. may be used to
+ * endorse or promote products derived from the Apple Software without specific
+ * prior written permission from Apple. Except as expressly stated in this
+ * notice, no other rights or licenses, express or implied, are granted by
+ * Apple herein, including but not limited to any patent rights that may be
+ * infringed by your derivative works or by other works in which the Apple
+ * Software may be incorporated.
+ * The Apple Software is provided by Apple on an "AS IS" basis. APPLE MAKES NO
+ * WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED
+ * WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN
+ * COMBINATION WITH YOUR PRODUCTS.
+ * IN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION
+ * AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER
+ * THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR
+ * OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#ifdef CONFIG_ARM64_TSO
+	bool        tso;
+#endif
 };
 
 static inline unsigned int thread_get_vl(struct thread_struct *thread,
 					 enum vec_type type)
 {
 	return thread->vl[type];
 }
 
 static inline unsigned int thread_get_sve_vl(struct thread_struct *thread)
 {
 	return thread_get_vl(thread, ARM64_VEC_SVE);
 }
 
 static inline unsigned int thread_get_sme_vl(struct thread_struct *thread)
 {
 	return thread_get_vl(thread, ARM64_VEC_SME);
 }
 
 static inline unsigned int thread_get_cur_vl(struct thread_struct *thread)
 {
 	if (system_supports_sme() && (thread->svcr & SVCR_SM_MASK))
 		return thread_get_sme_vl(thread);
 	else
 		return thread_get_sve_vl(thread);
 }
 
 unsigned int task_get_vl(const struct task_struct *task, enum vec_type type);
 void task_set_vl(struct task_struct *task, enum vec_type type,
 		 unsigned long vl);
 void task_set_vl_onexec(struct task_struct *task, enum vec_type type,
 			unsigned long vl);
 unsigned int task_get_vl_onexec(const struct task_struct *task,
 				enum vec_type type);
 
 static inline unsigned int task_get_sve_vl(const struct task_struct *task)
 {
 	return task_get_vl(task, ARM64_VEC_SVE);
 }
 
 static inline unsigned int task_get_sme_vl(const struct task_struct *task)
 {
 	return task_get_vl(task, ARM64_VEC_SME);
 }
 
 static inline void task_set_sve_vl(struct task_struct *task, unsigned long vl)
 {
 	task_set_vl(task, ARM64_VEC_SVE, vl);
 }
 
 static inline unsigned int task_get_sve_vl_onexec(const struct task_struct *task)
 {
 	return task_get_vl_onexec(task, ARM64_VEC_SVE);
 }
 
 static inline void task_set_sve_vl_onexec(struct task_struct *task,
 					  unsigned long vl)
 {
 	task_set_vl_onexec(task, ARM64_VEC_SVE, vl);
 }
 
 #define SCTLR_USER_MASK                                                        \
 	(SCTLR_ELx_ENIA | SCTLR_ELx_ENIB | SCTLR_ELx_ENDA | SCTLR_ELx_ENDB |   \
 	 SCTLR_EL1_TCF0_MASK)
 
 static inline void arch_thread_struct_whitelist(unsigned long *offset,
 						unsigned long *size)
 {
 	/* Verify that there is no padding among the whitelisted fields: */
 	BUILD_BUG_ON(sizeof_field(struct thread_struct, uw) !=
 		     sizeof_field(struct thread_struct, uw.tp_value) +
 		     sizeof_field(struct thread_struct, uw.tp2_value) +
 		     sizeof_field(struct thread_struct, uw.fpmr) +
 		     sizeof_field(struct thread_struct, uw.pad) +
 		     sizeof_field(struct thread_struct, uw.fpsimd_state));
 
 	*offset = offsetof(struct thread_struct, uw);
 	*size = sizeof_field(struct thread_struct, uw);
 }
 
 #ifdef CONFIG_COMPAT
 #define task_user_tls(t)						\
 ({									\
 	unsigned long *__tls;						\
 	if (is_compat_thread(task_thread_info(t)))			\
 		__tls = &(t)->thread.uw.tp2_value;			\
 	else								\
 		__tls = &(t)->thread.uw.tp_value;			\
 	__tls;								\
  })
 #else
 #define task_user_tls(t)	(&(t)->thread.uw.tp_value)
 #endif
 
 /* Sync TPIDR_EL0 back to thread_struct for current */
 void tls_preserve_current_state(void);
 
 #define INIT_THREAD {				\
 	.fpsimd_cpu = NR_CPUS,			\
 }
 
diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index b8303a83c0bf..176dafe322c8 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -182,336 +182,377 @@
 
 #define SYS_OSLSR_EL1			sys_reg(2, 0, 1, 1, 4)
 #define OSLSR_EL1_OSLM_MASK		(BIT(3) | BIT(0))
 #define OSLSR_EL1_OSLM_NI		0
 #define OSLSR_EL1_OSLM_IMPLEMENTED	BIT(3)
 #define OSLSR_EL1_OSLK			BIT(1)
 
 #define SYS_OSDLR_EL1			sys_reg(2, 0, 1, 3, 4)
 #define SYS_DBGPRCR_EL1			sys_reg(2, 0, 1, 4, 4)
 #define SYS_DBGCLAIMSET_EL1		sys_reg(2, 0, 7, 8, 6)
 #define SYS_DBGCLAIMCLR_EL1		sys_reg(2, 0, 7, 9, 6)
 #define SYS_DBGAUTHSTATUS_EL1		sys_reg(2, 0, 7, 14, 6)
 #define SYS_MDCCSR_EL0			sys_reg(2, 3, 0, 1, 0)
 #define SYS_DBGDTR_EL0			sys_reg(2, 3, 0, 4, 0)
 #define SYS_DBGDTRRX_EL0		sys_reg(2, 3, 0, 5, 0)
 #define SYS_DBGDTRTX_EL0		sys_reg(2, 3, 0, 5, 0)
 #define SYS_DBGVCR32_EL2		sys_reg(2, 4, 0, 7, 0)
 
 #define SYS_BRBINF_EL1(n)		sys_reg(2, 1, 8, (n & 15), (((n & 16) >> 2) | 0))
 #define SYS_BRBINFINJ_EL1		sys_reg(2, 1, 9, 1, 0)
 #define SYS_BRBSRC_EL1(n)		sys_reg(2, 1, 8, (n & 15), (((n & 16) >> 2) | 1))
 #define SYS_BRBSRCINJ_EL1		sys_reg(2, 1, 9, 1, 1)
 #define SYS_BRBTGT_EL1(n)		sys_reg(2, 1, 8, (n & 15), (((n & 16) >> 2) | 2))
 #define SYS_BRBTGTINJ_EL1		sys_reg(2, 1, 9, 1, 2)
 #define SYS_BRBTS_EL1			sys_reg(2, 1, 9, 0, 2)
 
 #define SYS_BRBCR_EL1			sys_reg(2, 1, 9, 0, 0)
 #define SYS_BRBFCR_EL1			sys_reg(2, 1, 9, 0, 1)
 #define SYS_BRBIDR0_EL1			sys_reg(2, 1, 9, 2, 0)
 
 #define SYS_TRCITECR_EL1		sys_reg(3, 0, 1, 2, 3)
 #define SYS_TRCACATR(m)			sys_reg(2, 1, 2, ((m & 7) << 1), (2 | (m >> 3)))
 #define SYS_TRCACVR(m)			sys_reg(2, 1, 2, ((m & 7) << 1), (0 | (m >> 3)))
 #define SYS_TRCAUTHSTATUS		sys_reg(2, 1, 7, 14, 6)
 #define SYS_TRCAUXCTLR			sys_reg(2, 1, 0, 6, 0)
 #define SYS_TRCBBCTLR			sys_reg(2, 1, 0, 15, 0)
 #define SYS_TRCCCCTLR			sys_reg(2, 1, 0, 14, 0)
 #define SYS_TRCCIDCCTLR0		sys_reg(2, 1, 3, 0, 2)
 #define SYS_TRCCIDCCTLR1		sys_reg(2, 1, 3, 1, 2)
 #define SYS_TRCCIDCVR(m)		sys_reg(2, 1, 3, ((m & 7) << 1), 0)
 #define SYS_TRCCLAIMCLR			sys_reg(2, 1, 7, 9, 6)
 #define SYS_TRCCLAIMSET			sys_reg(2, 1, 7, 8, 6)
 #define SYS_TRCCNTCTLR(m)		sys_reg(2, 1, 0, (4 | (m & 3)), 5)
 #define SYS_TRCCNTRLDVR(m)		sys_reg(2, 1, 0, (0 | (m & 3)), 5)
 #define SYS_TRCCNTVR(m)			sys_reg(2, 1, 0, (8 | (m & 3)), 5)
 #define SYS_TRCCONFIGR			sys_reg(2, 1, 0, 4, 0)
 #define SYS_TRCDEVARCH			sys_reg(2, 1, 7, 15, 6)
 #define SYS_TRCDEVID			sys_reg(2, 1, 7, 2, 7)
 #define SYS_TRCEVENTCTL0R		sys_reg(2, 1, 0, 8, 0)
 #define SYS_TRCEVENTCTL1R		sys_reg(2, 1, 0, 9, 0)
 #define SYS_TRCEXTINSELR(m)		sys_reg(2, 1, 0, (8 | (m & 3)), 4)
 #define SYS_TRCIDR0			sys_reg(2, 1, 0, 8, 7)
 #define SYS_TRCIDR10			sys_reg(2, 1, 0, 2, 6)
 #define SYS_TRCIDR11			sys_reg(2, 1, 0, 3, 6)
 #define SYS_TRCIDR12			sys_reg(2, 1, 0, 4, 6)
 #define SYS_TRCIDR13			sys_reg(2, 1, 0, 5, 6)
 #define SYS_TRCIDR1			sys_reg(2, 1, 0, 9, 7)
 #define SYS_TRCIDR2			sys_reg(2, 1, 0, 10, 7)
 #define SYS_TRCIDR3			sys_reg(2, 1, 0, 11, 7)
 #define SYS_TRCIDR4			sys_reg(2, 1, 0, 12, 7)
 #define SYS_TRCIDR5			sys_reg(2, 1, 0, 13, 7)
 #define SYS_TRCIDR6			sys_reg(2, 1, 0, 14, 7)
 #define SYS_TRCIDR7			sys_reg(2, 1, 0, 15, 7)
 #define SYS_TRCIDR8			sys_reg(2, 1, 0, 0, 6)
 #define SYS_TRCIDR9			sys_reg(2, 1, 0, 1, 6)
 #define SYS_TRCIMSPEC(m)		sys_reg(2, 1, 0, (m & 7), 7)
 #define SYS_TRCITEEDCR			sys_reg(2, 1, 0, 2, 1)
 #define SYS_TRCOSLSR			sys_reg(2, 1, 1, 1, 4)
 #define SYS_TRCPRGCTLR			sys_reg(2, 1, 0, 1, 0)
 #define SYS_TRCQCTLR			sys_reg(2, 1, 0, 1, 1)
 #define SYS_TRCRSCTLR(m)		sys_reg(2, 1, 1, (m & 15), (0 | (m >> 4)))
 #define SYS_TRCRSR			sys_reg(2, 1, 0, 10, 0)
 #define SYS_TRCSEQEVR(m)		sys_reg(2, 1, 0, (m & 3), 4)
 #define SYS_TRCSEQRSTEVR		sys_reg(2, 1, 0, 6, 4)
 #define SYS_TRCSEQSTR			sys_reg(2, 1, 0, 7, 4)
 #define SYS_TRCSSCCR(m)			sys_reg(2, 1, 1, (m & 7), 2)
 #define SYS_TRCSSCSR(m)			sys_reg(2, 1, 1, (8 | (m & 7)), 2)
 #define SYS_TRCSSPCICR(m)		sys_reg(2, 1, 1, (m & 7), 3)
 #define SYS_TRCSTALLCTLR		sys_reg(2, 1, 0, 11, 0)
 #define SYS_TRCSTATR			sys_reg(2, 1, 0, 3, 0)
 #define SYS_TRCSYNCPR			sys_reg(2, 1, 0, 13, 0)
 #define SYS_TRCTRACEIDR			sys_reg(2, 1, 0, 0, 1)
 #define SYS_TRCTSCTLR			sys_reg(2, 1, 0, 12, 0)
 #define SYS_TRCVICTLR			sys_reg(2, 1, 0, 0, 2)
 #define SYS_TRCVIIECTLR			sys_reg(2, 1, 0, 1, 2)
 #define SYS_TRCVIPCSSCTLR		sys_reg(2, 1, 0, 3, 2)
 #define SYS_TRCVISSCTLR			sys_reg(2, 1, 0, 2, 2)
 #define SYS_TRCVMIDCCTLR0		sys_reg(2, 1, 3, 2, 2)
 #define SYS_TRCVMIDCCTLR1		sys_reg(2, 1, 3, 3, 2)
 #define SYS_TRCVMIDCVR(m)		sys_reg(2, 1, 3, ((m & 7) << 1), 1)
 
 /* ETM */
 #define SYS_TRCOSLAR			sys_reg(2, 1, 1, 0, 4)
 
 #define SYS_BRBCR_EL2			sys_reg(2, 4, 9, 0, 0)
 
 #define SYS_MIDR_EL1			sys_reg(3, 0, 0, 0, 0)
 #define SYS_MPIDR_EL1			sys_reg(3, 0, 0, 0, 5)
 #define SYS_REVIDR_EL1			sys_reg(3, 0, 0, 0, 6)
 
+/*
+ * Copyright © 2024 Apple Inc. All Rights Reserved.
+ * Disclaimer: IMPORTANT: This Apple software is supplied to you by Apple Inc.
+ * ("Apple") in consideration of your agreement to the following terms, and
+ * your use, installation, modification or redistribution of this Apple
+ * software constitutes acceptance of these terms. If you do not agree with
+ * these terms, please do not use, install, modify or redistribute this Apple
+ * software.
+ * In consideration of your agreement to abide by the following terms, and
+ * subject to these terms, Apple grants you a personal, non-exclusive license,
+ * under Apple's copyrights in this original Apple software (the "Apple
+ * Software"), to use, reproduce, modify and redistribute the Apple Software,
+ * with or without modifications, in source and/or binary forms; provided that
+ * if you redistribute the Apple Software in its entirety and without
+ * modifications, you must retain this notice and the following text and
+ * disclaimers in all such redistributions of the Apple Software. Neither the
+ * name, trademarks, service marks or logos of Apple Inc. may be used to
+ * endorse or promote products derived from the Apple Software without specific
+ * prior written permission from Apple. Except as expressly stated in this
+ * notice, no other rights or licenses, express or implied, are granted by
+ * Apple herein, including but not limited to any patent rights that may be
+ * infringed by your derivative works or by other works in which the Apple
+ * Software may be incorporated.
+ * The Apple Software is provided by Apple on an "AS IS" basis. APPLE MAKES NO
+ * WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED
+ * WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN
+ * COMBINATION WITH YOUR PRODUCTS.
+ * IN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION
+ * AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER
+ * THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR
+ * OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
 #define SYS_ACTLR_EL1			sys_reg(3, 0, 1, 0, 1)
+#define SYS_ACTLR_EL1_TSOEN_SHIFT       1
+#define SYS_ACTLR_EL1_TSOEN_MASK        (1 << SYS_ACTLR_EL1_TSOEN_SHIFT)
+
 #define SYS_RGSR_EL1			sys_reg(3, 0, 1, 0, 5)
 #define SYS_GCR_EL1			sys_reg(3, 0, 1, 0, 6)
 
 #define SYS_TRFCR_EL1			sys_reg(3, 0, 1, 2, 1)
 
 #define SYS_TCR_EL1			sys_reg(3, 0, 2, 0, 2)
 
 #define SYS_APIAKEYLO_EL1		sys_reg(3, 0, 2, 1, 0)
 #define SYS_APIAKEYHI_EL1		sys_reg(3, 0, 2, 1, 1)
 #define SYS_APIBKEYLO_EL1		sys_reg(3, 0, 2, 1, 2)
 #define SYS_APIBKEYHI_EL1		sys_reg(3, 0, 2, 1, 3)
 
 #define SYS_APDAKEYLO_EL1		sys_reg(3, 0, 2, 2, 0)
 #define SYS_APDAKEYHI_EL1		sys_reg(3, 0, 2, 2, 1)
 #define SYS_APDBKEYLO_EL1		sys_reg(3, 0, 2, 2, 2)
 #define SYS_APDBKEYHI_EL1		sys_reg(3, 0, 2, 2, 3)
 
 #define SYS_APGAKEYLO_EL1		sys_reg(3, 0, 2, 3, 0)
 #define SYS_APGAKEYHI_EL1		sys_reg(3, 0, 2, 3, 1)
 
 #define SYS_SPSR_EL1			sys_reg(3, 0, 4, 0, 0)
 #define SYS_ELR_EL1			sys_reg(3, 0, 4, 0, 1)
 
 #define SYS_ICC_PMR_EL1			sys_reg(3, 0, 4, 6, 0)
 
 #define SYS_AFSR0_EL1			sys_reg(3, 0, 5, 1, 0)
 #define SYS_AFSR1_EL1			sys_reg(3, 0, 5, 1, 1)
 #define SYS_ESR_EL1			sys_reg(3, 0, 5, 2, 0)
 
 #define SYS_ERRIDR_EL1			sys_reg(3, 0, 5, 3, 0)
 #define SYS_ERRSELR_EL1			sys_reg(3, 0, 5, 3, 1)
 #define SYS_ERXFR_EL1			sys_reg(3, 0, 5, 4, 0)
 #define SYS_ERXCTLR_EL1			sys_reg(3, 0, 5, 4, 1)
 #define SYS_ERXSTATUS_EL1		sys_reg(3, 0, 5, 4, 2)
 #define SYS_ERXADDR_EL1			sys_reg(3, 0, 5, 4, 3)
 #define SYS_ERXPFGF_EL1			sys_reg(3, 0, 5, 4, 4)
 #define SYS_ERXPFGCTL_EL1		sys_reg(3, 0, 5, 4, 5)
 #define SYS_ERXPFGCDN_EL1		sys_reg(3, 0, 5, 4, 6)
 #define SYS_ERXMISC0_EL1		sys_reg(3, 0, 5, 5, 0)
 #define SYS_ERXMISC1_EL1		sys_reg(3, 0, 5, 5, 1)
 #define SYS_ERXMISC2_EL1		sys_reg(3, 0, 5, 5, 2)
 #define SYS_ERXMISC3_EL1		sys_reg(3, 0, 5, 5, 3)
 #define SYS_TFSR_EL1			sys_reg(3, 0, 5, 6, 0)
 #define SYS_TFSRE0_EL1			sys_reg(3, 0, 5, 6, 1)
 
 #define SYS_PAR_EL1			sys_reg(3, 0, 7, 4, 0)
 
 #define SYS_PAR_EL1_F			BIT(0)
 /* When PAR_EL1.F == 1 */
 #define SYS_PAR_EL1_FST			GENMASK(6, 1)
 #define SYS_PAR_EL1_PTW			BIT(8)
 #define SYS_PAR_EL1_S			BIT(9)
 #define SYS_PAR_EL1_AssuredOnly		BIT(12)
 #define SYS_PAR_EL1_TopLevel		BIT(13)
 #define SYS_PAR_EL1_Overlay		BIT(14)
 #define SYS_PAR_EL1_DirtyBit		BIT(15)
 #define SYS_PAR_EL1_F1_IMPDEF		GENMASK_ULL(63, 48)
 #define SYS_PAR_EL1_F1_RES0		(BIT(7) | BIT(10) | GENMASK_ULL(47, 16))
 #define SYS_PAR_EL1_RES1		BIT(11)
 /* When PAR_EL1.F == 0 */
 #define SYS_PAR_EL1_SH			GENMASK_ULL(8, 7)
 #define SYS_PAR_EL1_NS			BIT(9)
 #define SYS_PAR_EL1_F0_IMPDEF		BIT(10)
 #define SYS_PAR_EL1_NSE			BIT(11)
 #define SYS_PAR_EL1_PA			GENMASK_ULL(51, 12)
 #define SYS_PAR_EL1_ATTR		GENMASK_ULL(63, 56)
 #define SYS_PAR_EL1_F0_RES0		(GENMASK_ULL(6, 1) | GENMASK_ULL(55, 52))
 
 /*** Statistical Profiling Extension ***/
 #define PMSEVFR_EL1_RES0_IMP	\
 	(GENMASK_ULL(47, 32) | GENMASK_ULL(23, 16) | GENMASK_ULL(11, 8) |\
 	 BIT_ULL(6) | BIT_ULL(4) | BIT_ULL(2) | BIT_ULL(0))
 #define PMSEVFR_EL1_RES0_V1P1	\
 	(PMSEVFR_EL1_RES0_IMP & ~(BIT_ULL(18) | BIT_ULL(17) | BIT_ULL(11)))
 #define PMSEVFR_EL1_RES0_V1P2	\
 	(PMSEVFR_EL1_RES0_V1P1 & ~BIT_ULL(6))
 
 /* Buffer error reporting */
 #define PMBSR_EL1_FAULT_FSC_SHIFT	PMBSR_EL1_MSS_SHIFT
 #define PMBSR_EL1_FAULT_FSC_MASK	PMBSR_EL1_MSS_MASK
 
 #define PMBSR_EL1_BUF_BSC_SHIFT		PMBSR_EL1_MSS_SHIFT
 #define PMBSR_EL1_BUF_BSC_MASK		PMBSR_EL1_MSS_MASK
 
 #define PMBSR_EL1_BUF_BSC_FULL		0x1UL
 
 /*** End of Statistical Profiling Extension ***/
 
 #define TRBSR_EL1_BSC_MASK		GENMASK(5, 0)
 #define TRBSR_EL1_BSC_SHIFT		0
 
 #define SYS_PMINTENSET_EL1		sys_reg(3, 0, 9, 14, 1)
 #define SYS_PMINTENCLR_EL1		sys_reg(3, 0, 9, 14, 2)
 
 #define SYS_PMMIR_EL1			sys_reg(3, 0, 9, 14, 6)
 
 #define SYS_MAIR_EL1			sys_reg(3, 0, 10, 2, 0)
 #define SYS_AMAIR_EL1			sys_reg(3, 0, 10, 3, 0)
 
 #define SYS_VBAR_EL1			sys_reg(3, 0, 12, 0, 0)
 #define SYS_DISR_EL1			sys_reg(3, 0, 12, 1, 1)
 
 #define SYS_ICC_IAR0_EL1		sys_reg(3, 0, 12, 8, 0)
 #define SYS_ICC_EOIR0_EL1		sys_reg(3, 0, 12, 8, 1)
 #define SYS_ICC_HPPIR0_EL1		sys_reg(3, 0, 12, 8, 2)
 #define SYS_ICC_BPR0_EL1		sys_reg(3, 0, 12, 8, 3)
 #define SYS_ICC_AP0Rn_EL1(n)		sys_reg(3, 0, 12, 8, 4 | n)
 #define SYS_ICC_AP0R0_EL1		SYS_ICC_AP0Rn_EL1(0)
 #define SYS_ICC_AP0R1_EL1		SYS_ICC_AP0Rn_EL1(1)
 #define SYS_ICC_AP0R2_EL1		SYS_ICC_AP0Rn_EL1(2)
 #define SYS_ICC_AP0R3_EL1		SYS_ICC_AP0Rn_EL1(3)
 #define SYS_ICC_AP1Rn_EL1(n)		sys_reg(3, 0, 12, 9, n)
 #define SYS_ICC_AP1R0_EL1		SYS_ICC_AP1Rn_EL1(0)
 #define SYS_ICC_AP1R1_EL1		SYS_ICC_AP1Rn_EL1(1)
 #define SYS_ICC_AP1R2_EL1		SYS_ICC_AP1Rn_EL1(2)
 #define SYS_ICC_AP1R3_EL1		SYS_ICC_AP1Rn_EL1(3)
 #define SYS_ICC_DIR_EL1			sys_reg(3, 0, 12, 11, 1)
 #define SYS_ICC_RPR_EL1			sys_reg(3, 0, 12, 11, 3)
 #define SYS_ICC_SGI1R_EL1		sys_reg(3, 0, 12, 11, 5)
 #define SYS_ICC_ASGI1R_EL1		sys_reg(3, 0, 12, 11, 6)
 #define SYS_ICC_SGI0R_EL1		sys_reg(3, 0, 12, 11, 7)
 #define SYS_ICC_IAR1_EL1		sys_reg(3, 0, 12, 12, 0)
 #define SYS_ICC_EOIR1_EL1		sys_reg(3, 0, 12, 12, 1)
 #define SYS_ICC_HPPIR1_EL1		sys_reg(3, 0, 12, 12, 2)
 #define SYS_ICC_BPR1_EL1		sys_reg(3, 0, 12, 12, 3)
 #define SYS_ICC_CTLR_EL1		sys_reg(3, 0, 12, 12, 4)
 #define SYS_ICC_SRE_EL1			sys_reg(3, 0, 12, 12, 5)
 #define SYS_ICC_IGRPEN0_EL1		sys_reg(3, 0, 12, 12, 6)
 #define SYS_ICC_IGRPEN1_EL1		sys_reg(3, 0, 12, 12, 7)
 
 #define SYS_ACCDATA_EL1			sys_reg(3, 0, 13, 0, 5)
 
 #define SYS_CNTKCTL_EL1			sys_reg(3, 0, 14, 1, 0)
 
 #define SYS_AIDR_EL1			sys_reg(3, 1, 0, 0, 7)
+#define SYS_AIDR_EL1_TSO_SHIFT          9
+#define SYS_AIDR_EL1_TSO_MASK           (1 << SYS_AIDR_EL1_TSO_SHIFT)
 
 #define SYS_RNDR_EL0			sys_reg(3, 3, 2, 4, 0)
 #define SYS_RNDRRS_EL0			sys_reg(3, 3, 2, 4, 1)
 
 #define SYS_PMCR_EL0			sys_reg(3, 3, 9, 12, 0)
 #define SYS_PMCNTENSET_EL0		sys_reg(3, 3, 9, 12, 1)
 #define SYS_PMCNTENCLR_EL0		sys_reg(3, 3, 9, 12, 2)
 #define SYS_PMOVSCLR_EL0		sys_reg(3, 3, 9, 12, 3)
 #define SYS_PMSWINC_EL0			sys_reg(3, 3, 9, 12, 4)
 #define SYS_PMCEID0_EL0			sys_reg(3, 3, 9, 12, 6)
 #define SYS_PMCEID1_EL0			sys_reg(3, 3, 9, 12, 7)
 #define SYS_PMCCNTR_EL0			sys_reg(3, 3, 9, 13, 0)
 #define SYS_PMXEVTYPER_EL0		sys_reg(3, 3, 9, 13, 1)
 #define SYS_PMXEVCNTR_EL0		sys_reg(3, 3, 9, 13, 2)
 #define SYS_PMUSERENR_EL0		sys_reg(3, 3, 9, 14, 0)
 #define SYS_PMOVSSET_EL0		sys_reg(3, 3, 9, 14, 3)
 
 #define SYS_TPIDR_EL0			sys_reg(3, 3, 13, 0, 2)
 #define SYS_TPIDRRO_EL0			sys_reg(3, 3, 13, 0, 3)
 #define SYS_TPIDR2_EL0			sys_reg(3, 3, 13, 0, 5)
 
 #define SYS_SCXTNUM_EL0			sys_reg(3, 3, 13, 0, 7)
 
 /* Definitions for system register interface to AMU for ARMv8.4 onwards */
 #define SYS_AM_EL0(crm, op2)		sys_reg(3, 3, 13, (crm), (op2))
 #define SYS_AMCR_EL0			SYS_AM_EL0(2, 0)
 #define SYS_AMCFGR_EL0			SYS_AM_EL0(2, 1)
 #define SYS_AMCGCR_EL0			SYS_AM_EL0(2, 2)
 #define SYS_AMUSERENR_EL0		SYS_AM_EL0(2, 3)
 #define SYS_AMCNTENCLR0_EL0		SYS_AM_EL0(2, 4)
 #define SYS_AMCNTENSET0_EL0		SYS_AM_EL0(2, 5)
 #define SYS_AMCNTENCLR1_EL0		SYS_AM_EL0(3, 0)
 #define SYS_AMCNTENSET1_EL0		SYS_AM_EL0(3, 1)
 
 /*
  * Group 0 of activity monitors (architected):
  *                op0  op1  CRn   CRm       op2
  * Counter:       11   011  1101  010:n<3>  n<2:0>
  * Type:          11   011  1101  011:n<3>  n<2:0>
  * n: 0-15
  *
  * Group 1 of activity monitors (auxiliary):
  *                op0  op1  CRn   CRm       op2
  * Counter:       11   011  1101  110:n<3>  n<2:0>
  * Type:          11   011  1101  111:n<3>  n<2:0>
  * n: 0-15
  */
 
 #define SYS_AMEVCNTR0_EL0(n)		SYS_AM_EL0(4 + ((n) >> 3), (n) & 7)
 #define SYS_AMEVTYPER0_EL0(n)		SYS_AM_EL0(6 + ((n) >> 3), (n) & 7)
 #define SYS_AMEVCNTR1_EL0(n)		SYS_AM_EL0(12 + ((n) >> 3), (n) & 7)
 #define SYS_AMEVTYPER1_EL0(n)		SYS_AM_EL0(14 + ((n) >> 3), (n) & 7)
 
 /* AMU v1: Fixed (architecturally defined) activity monitors */
 #define SYS_AMEVCNTR0_CORE_EL0		SYS_AMEVCNTR0_EL0(0)
 #define SYS_AMEVCNTR0_CONST_EL0		SYS_AMEVCNTR0_EL0(1)
 #define SYS_AMEVCNTR0_INST_RET_EL0	SYS_AMEVCNTR0_EL0(2)
 #define SYS_AMEVCNTR0_MEM_STALL		SYS_AMEVCNTR0_EL0(3)
 
 #define SYS_CNTFRQ_EL0			sys_reg(3, 3, 14, 0, 0)
 
 #define SYS_CNTPCT_EL0			sys_reg(3, 3, 14, 0, 1)
 #define SYS_CNTPCTSS_EL0		sys_reg(3, 3, 14, 0, 5)
 #define SYS_CNTVCTSS_EL0		sys_reg(3, 3, 14, 0, 6)
 
 #define SYS_CNTP_TVAL_EL0		sys_reg(3, 3, 14, 2, 0)
 #define SYS_CNTP_CTL_EL0		sys_reg(3, 3, 14, 2, 1)
 #define SYS_CNTP_CVAL_EL0		sys_reg(3, 3, 14, 2, 2)
 
 #define SYS_CNTV_CTL_EL0		sys_reg(3, 3, 14, 3, 1)
 #define SYS_CNTV_CVAL_EL0		sys_reg(3, 3, 14, 3, 2)
 
 #define SYS_AARCH32_CNTP_TVAL		sys_reg(0, 0, 14, 2, 0)
 #define SYS_AARCH32_CNTP_CTL		sys_reg(0, 0, 14, 2, 1)
 #define SYS_AARCH32_CNTPCT		sys_reg(0, 0, 0, 14, 0)
 #define SYS_AARCH32_CNTP_CVAL		sys_reg(0, 2, 0, 14, 0)
 #define SYS_AARCH32_CNTPCTSS		sys_reg(0, 8, 0, 14, 0)
 
 #define __PMEV_op2(n)			((n) & 0x7)
 #define __CNTR_CRm(n)			(0x8 | (((n) >> 3) & 0x3))
 #define SYS_PMEVCNTRn_EL0(n)		sys_reg(3, 3, 14, __CNTR_CRm(n), __PMEV_op2(n))
 #define __TYPER_CRm(n)			(0xc | (((n) >> 3) & 0x3))
 #define SYS_PMEVTYPERn_EL0(n)		sys_reg(3, 3, 14, __TYPER_CRm(n), __PMEV_op2(n))
 
 #define SYS_PMCCFILTR_EL0		sys_reg(3, 3, 14, 15, 7)
 
 #define SYS_VPIDR_EL2			sys_reg(3, 4, 0, 0, 0)
 #define SYS_VMPIDR_EL2			sys_reg(3, 4, 0, 0, 5)
 
 #define SYS_SCTLR_EL2			sys_reg(3, 4, 1, 0, 0)
 #define SYS_ACTLR_EL2			sys_reg(3, 4, 1, 0, 1)
 #define SYS_SCTLR2_EL2			sys_reg(3, 4, 1, 0, 3)
 #define SYS_HCR_EL2			sys_reg(3, 4, 1, 1, 0)
 #define SYS_MDCR_EL2			sys_reg(3, 4, 1, 1, 1)
 #define SYS_CPTR_EL2			sys_reg(3, 4, 1, 1, 2)
 #define SYS_HSTR_EL2			sys_reg(3, 4, 1, 1, 3)
 #define SYS_HACR_EL2			sys_reg(3, 4, 1, 1, 7)
 
 #define SYS_TTBR0_EL2			sys_reg(3, 4, 2, 0, 0)
 #define SYS_TTBR1_EL2			sys_reg(3, 4, 2, 0, 1)
diff --git a/arch/arm64/include/asm/tso.h b/arch/arm64/include/asm/tso.h
new file mode 100644
index 000000000000..ae97f6a2a0d5
--- /dev/null
+++ b/arch/arm64/include/asm/tso.h
@@ -0,0 +1,52 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Copyright © 2024 Apple Inc. All Rights Reserved.
+ * Disclaimer: IMPORTANT: This Apple software is supplied to you by Apple Inc.
+ * ("Apple") in consideration of your agreement to the following terms, and
+ * your use, installation, modification or redistribution of this Apple
+ * software constitutes acceptance of these terms. If you do not agree with
+ * these terms, please do not use, install, modify or redistribute this Apple
+ * software.
+ * In consideration of your agreement to abide by the following terms, and
+ * subject to these terms, Apple grants you a personal, non-exclusive license,
+ * under Apple's copyrights in this original Apple software (the "Apple
+ * Software"), to use, reproduce, modify and redistribute the Apple Software,
+ * with or without modifications, in source and/or binary forms; provided that
+ * if you redistribute the Apple Software in its entirety and without
+ * modifications, you must retain this notice and the following text and
+ * disclaimers in all such redistributions of the Apple Software. Neither the
+ * name, trademarks, service marks or logos of Apple Inc. may be used to
+ * endorse or promote products derived from the Apple Software without specific
+ * prior written permission from Apple. Except as expressly stated in this
+ * notice, no other rights or licenses, express or implied, are granted by
+ * Apple herein, including but not limited to any patent rights that may be
+ * infringed by your derivative works or by other works in which the Apple
+ * Software may be incorporated.
+ * The Apple Software is provided by Apple on an "AS IS" basis. APPLE MAKES NO
+ * WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED
+ * WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN
+ * COMBINATION WITH YOUR PRODUCTS.
+ * IN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION
+ * AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER
+ * THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR
+ * OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __ASM_TSO_H
+#define __ASM_TSO_H
+
+#ifdef CONFIG_ARM64_TSO
+
+#include <linux/sched.h>
+#include <linux/types.h>
+
+int modify_tso_enable(bool tso_enable);
+void tso_thread_switch(struct task_struct *next);
+int arch_set_mem_model(struct task_struct *task, int memory_model);
+
+#endif /* CONFIG_ARM64_TSO */
+#endif /* __ASM_TSO_H */
diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
index 71c29a2a2f19..cafafef2a5b0 100644
--- a/arch/arm64/kernel/Makefile
+++ b/arch/arm64/kernel/Makefile
@@ -1,88 +1,88 @@
 # SPDX-License-Identifier: GPL-2.0
 #
 # Makefile for the linux kernel.
 #
 
 CFLAGS_armv8_deprecated.o := -I$(src)
 
 CFLAGS_REMOVE_ftrace.o = $(CC_FLAGS_FTRACE)
 CFLAGS_REMOVE_insn.o = $(CC_FLAGS_FTRACE)
 CFLAGS_REMOVE_return_address.o = $(CC_FLAGS_FTRACE)
 
 # Remove stack protector to avoid triggering unneeded stack canary
 # checks due to randomize_kstack_offset.
 CFLAGS_REMOVE_syscall.o	 = -fstack-protector -fstack-protector-strong
 CFLAGS_syscall.o	+= -fno-stack-protector
 
 # When KASAN is enabled, a stack trace is recorded for every alloc/free, which
 # can significantly impact performance. Avoid instrumenting the stack trace
 # collection code to minimize this impact.
 KASAN_SANITIZE_stacktrace.o := n
 
 # It's not safe to invoke KCOV when portions of the kernel environment aren't
 # available or are out-of-sync with HW state. Since `noinstr` doesn't always
 # inhibit KCOV instrumentation, disable it for the entire compilation unit.
 KCOV_INSTRUMENT_entry-common.o := n
 KCOV_INSTRUMENT_idle.o := n
 
 # Object file lists.
 obj-y			:= debug-monitors.o entry.o irq.o fpsimd.o		\
 			   entry-common.o entry-fpsimd.o process.o ptrace.o	\
 			   setup.o signal.o sys.o stacktrace.o time.o traps.o	\
 			   io.o vdso.o hyp-stub.o psci.o cpu_ops.o		\
 			   return_address.o cpuinfo.o cpu_errata.o		\
 			   cpufeature.o alternative.o cacheinfo.o		\
 			   smp.o smp_spin_table.o topology.o smccc-call.o	\
-			   syscall.o proton-pack.o idle.o patching.o pi/	\
+			   syscall.o proton-pack.o idle.o patching.o tso.o pi/	\
 			   rsi.o
 
 obj-$(CONFIG_COMPAT)			+= sys32.o signal32.o			\
 					   sys_compat.o
 obj-$(CONFIG_COMPAT)			+= sigreturn32.o
 obj-$(CONFIG_COMPAT_ALIGNMENT_FIXUPS)	+= compat_alignment.o
 obj-$(CONFIG_KUSER_HELPERS)		+= kuser32.o
 obj-$(CONFIG_FUNCTION_TRACER)		+= ftrace.o entry-ftrace.o
 obj-$(CONFIG_MODULES)			+= module.o module-plts.o
 obj-$(CONFIG_PERF_EVENTS)		+= perf_regs.o perf_callchain.o
 obj-$(CONFIG_HARDLOCKUP_DETECTOR_PERF)	+= watchdog_hld.o
 obj-$(CONFIG_HAVE_HW_BREAKPOINT)	+= hw_breakpoint.o
 obj-$(CONFIG_CPU_PM)			+= sleep.o suspend.o
 obj-$(CONFIG_JUMP_LABEL)		+= jump_label.o
 obj-$(CONFIG_KGDB)			+= kgdb.o
 obj-$(CONFIG_EFI)			+= efi.o efi-rt-wrapper.o
 obj-$(CONFIG_PCI)			+= pci.o
 obj-$(CONFIG_ARMV8_DEPRECATED)		+= armv8_deprecated.o
 obj-$(CONFIG_ACPI)			+= acpi.o
 obj-$(CONFIG_ACPI_NUMA)			+= acpi_numa.o
 obj-$(CONFIG_ARM64_ACPI_PARKING_PROTOCOL)	+= acpi_parking_protocol.o
 obj-$(CONFIG_PARAVIRT)			+= paravirt.o
 obj-$(CONFIG_RANDOMIZE_BASE)		+= kaslr.o
 obj-$(CONFIG_HIBERNATION)		+= hibernate.o hibernate-asm.o
 obj-$(CONFIG_ELF_CORE)			+= elfcore.o
 obj-$(CONFIG_KEXEC_CORE)		+= machine_kexec.o relocate_kernel.o	\
 					   cpu-reset.o
 obj-$(CONFIG_KEXEC_FILE)		+= machine_kexec_file.o kexec_image.o
 obj-$(CONFIG_ARM64_RELOC_TEST)		+= arm64-reloc-test.o
 arm64-reloc-test-y := reloc_test_core.o reloc_test_syms.o
 obj-$(CONFIG_CRASH_DUMP)		+= crash_dump.o
 obj-$(CONFIG_VMCORE_INFO)		+= vmcore_info.o
 obj-$(CONFIG_ARM_SDE_INTERFACE)		+= sdei.o
 obj-$(CONFIG_ARM64_PTR_AUTH)		+= pointer_auth.o
 obj-$(CONFIG_ARM64_MTE)			+= mte.o
 obj-y					+= vdso-wrap.o
 obj-$(CONFIG_COMPAT_VDSO)		+= vdso32-wrap.o
 
 # Force dependency (vdso*-wrap.S includes vdso.so through incbin)
 $(obj)/vdso-wrap.o: $(obj)/vdso/vdso.so
 $(obj)/vdso32-wrap.o: $(obj)/vdso32/vdso.so
 
 obj-y					+= probes/
 obj-y					+= head.o
 extra-y					+= vmlinux.lds
 
 ifeq ($(CONFIG_DEBUG_EFI),y)
 AFLAGS_head.o += -DVMLINUX_PATH="\"$(realpath $(objtree)/vmlinux)\""
 endif
 
 # for cleaning
 subdir- += vdso vdso32
diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 2968a33bb3bc..33e66aaf393e 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -1,159 +1,196 @@
 // SPDX-License-Identifier: GPL-2.0-only
 /*
  * Based on arch/arm/kernel/process.c
  *
  * Original Copyright (C) 1995  Linus Torvalds
  * Copyright (C) 1996-2000 Russell King - Converted to ARM.
  * Copyright (C) 2012 ARM Ltd.
  */
 #include <linux/compat.h>
 #include <linux/efi.h>
 #include <linux/elf.h>
 #include <linux/export.h>
 #include <linux/sched.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
 #include <linux/kernel.h>
 #include <linux/mman.h>
 #include <linux/mm.h>
 #include <linux/nospec.h>
 #include <linux/stddef.h>
 #include <linux/sysctl.h>
 #include <linux/unistd.h>
 #include <linux/user.h>
 #include <linux/delay.h>
 #include <linux/reboot.h>
 #include <linux/interrupt.h>
 #include <linux/init.h>
 #include <linux/cpu.h>
 #include <linux/elfcore.h>
 #include <linux/pm.h>
 #include <linux/tick.h>
 #include <linux/utsname.h>
 #include <linux/uaccess.h>
 #include <linux/random.h>
 #include <linux/hw_breakpoint.h>
 #include <linux/personality.h>
 #include <linux/notifier.h>
 #include <trace/events/power.h>
 #include <linux/percpu.h>
 #include <linux/thread_info.h>
 #include <linux/prctl.h>
 #include <linux/stacktrace.h>
 
 #include <asm/alternative.h>
 #include <asm/arch_timer.h>
 #include <asm/compat.h>
 #include <asm/cpufeature.h>
 #include <asm/cacheflush.h>
 #include <asm/exec.h>
 #include <asm/fpsimd.h>
 #include <asm/gcs.h>
 #include <asm/mmu_context.h>
 #include <asm/mte.h>
 #include <asm/processor.h>
 #include <asm/pointer_auth.h>
 #include <asm/stacktrace.h>
 #include <asm/switch_to.h>
 #include <asm/system_misc.h>
+/*
+ * Copyright © 2024 Apple Inc. All Rights Reserved.
+ * Disclaimer: IMPORTANT: This Apple software is supplied to you by Apple Inc.
+ * ("Apple") in consideration of your agreement to the following terms, and
+ * your use, installation, modification or redistribution of this Apple
+ * software constitutes acceptance of these terms. If you do not agree with
+ * these terms, please do not use, install, modify or redistribute this Apple
+ * software.
+ * In consideration of your agreement to abide by the following terms, and
+ * subject to these terms, Apple grants you a personal, non-exclusive license,
+ * under Apple's copyrights in this original Apple software (the "Apple
+ * Software"), to use, reproduce, modify and redistribute the Apple Software,
+ * with or without modifications, in source and/or binary forms; provided that
+ * if you redistribute the Apple Software in its entirety and without
+ * modifications, you must retain this notice and the following text and
+ * disclaimers in all such redistributions of the Apple Software. Neither the
+ * name, trademarks, service marks or logos of Apple Inc. may be used to
+ * endorse or promote products derived from the Apple Software without specific
+ * prior written permission from Apple. Except as expressly stated in this
+ * notice, no other rights or licenses, express or implied, are granted by
+ * Apple herein, including but not limited to any patent rights that may be
+ * infringed by your derivative works or by other works in which the Apple
+ * Software may be incorporated.
+ * The Apple Software is provided by Apple on an "AS IS" basis. APPLE MAKES NO
+ * WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED
+ * WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN
+ * COMBINATION WITH YOUR PRODUCTS.
+ * IN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION
+ * AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER
+ * THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR
+ * OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#include <asm/tso.h>
 
 #if defined(CONFIG_STACKPROTECTOR) && !defined(CONFIG_STACKPROTECTOR_PER_TASK)
 #include <linux/stackprotector.h>
 unsigned long __stack_chk_guard __ro_after_init;
 EXPORT_SYMBOL(__stack_chk_guard);
 #endif
 
 /*
  * Function pointers to optional machine specific functions
  */
 void (*pm_power_off)(void);
 EXPORT_SYMBOL_GPL(pm_power_off);
 
 #ifdef CONFIG_HOTPLUG_CPU
 void __noreturn arch_cpu_idle_dead(void)
 {
        cpu_die();
 }
 #endif
 
 /*
  * Called by kexec, immediately prior to machine_kexec().
  *
  * This must completely disable all secondary CPUs; simply causing those CPUs
  * to execute e.g. a RAM-based pin loop is not sufficient. This allows the
  * kexec'd kernel to use any and all RAM as it sees fit, without having to
  * avoid any code or data used by any SW CPU pin loop. The CPU hotplug
  * functionality embodied in smpt_shutdown_nonboot_cpus() to achieve this.
  */
 void machine_shutdown(void)
 {
 	smp_shutdown_nonboot_cpus(reboot_cpu);
 }
 
 /*
  * Halting simply requires that the secondary CPUs stop performing any
  * activity (executing tasks, handling interrupts). smp_send_stop()
  * achieves this.
  */
 void machine_halt(void)
 {
 	local_irq_disable();
 	smp_send_stop();
 	while (1);
 }
 
 /*
  * Power-off simply requires that the secondary CPUs stop performing any
  * activity (executing tasks, handling interrupts). smp_send_stop()
  * achieves this. When the system power is turned off, it will take all CPUs
  * with it.
  */
 void machine_power_off(void)
 {
 	local_irq_disable();
 	smp_send_stop();
 	do_kernel_power_off();
 }
 
 /*
  * Restart requires that the secondary CPUs stop performing any activity
  * while the primary CPU resets the system. Systems with multiple CPUs must
  * provide a HW restart implementation, to ensure that all CPUs reset at once.
  * This is required so that any code running after reset on the primary CPU
  * doesn't have to co-ordinate with other CPUs to ensure they aren't still
  * executing pre-reset code, and using RAM that the primary CPU's code wishes
  * to use. Implementing such co-ordination would be essentially impossible.
  */
 void machine_restart(char *cmd)
 {
 	/* Disable interrupts first */
 	local_irq_disable();
 	smp_send_stop();
 
 	/*
 	 * UpdateCapsule() depends on the system being reset via
 	 * ResetSystem().
 	 */
 	if (efi_enabled(EFI_RUNTIME_SERVICES))
 		efi_reboot(reboot_mode, NULL);
 
 	/* Now call the architecture specific reboot code. */
 	do_kernel_restart(cmd);
 
 	/*
 	 * Whoops - the architecture was unable to reboot.
 	 */
 	printk("Reboot failed -- System halted\n");
 	while (1);
 }
 
 #define bstr(suffix, str) [PSR_BTYPE_ ## suffix >> PSR_BTYPE_SHIFT] = str
 static const char *const btypes[] = {
 	bstr(NONE, "--"),
 	bstr(  JC, "jc"),
 	bstr(   C, "-c"),
 	bstr(  J , "j-")
 };
 #undef bstr
 
@@ -581,347 +618,406 @@ static void gcs_thread_switch(struct task_struct *next)
 
 #endif
 
 /*
  * Handle sysreg updates for ARM erratum 1418040 which affects the 32bit view of
  * CNTVCT, various other errata which require trapping all CNTVCT{,_EL0}
  * accesses and prctl(PR_SET_TSC). Ensure access is disabled iff a workaround is
  * required or PR_TSC_SIGSEGV is set.
  */
 static void update_cntkctl_el1(struct task_struct *next)
 {
 	struct thread_info *ti = task_thread_info(next);
 
 	if (test_ti_thread_flag(ti, TIF_TSC_SIGSEGV) ||
 	    has_erratum_handler(read_cntvct_el0) ||
 	    (IS_ENABLED(CONFIG_ARM64_ERRATUM_1418040) &&
 	     this_cpu_has_cap(ARM64_WORKAROUND_1418040) &&
 	     is_compat_thread(ti)))
 		sysreg_clear_set(cntkctl_el1, ARCH_TIMER_USR_VCT_ACCESS_EN, 0);
 	else
 		sysreg_clear_set(cntkctl_el1, 0, ARCH_TIMER_USR_VCT_ACCESS_EN);
 }
 
 static void cntkctl_thread_switch(struct task_struct *prev,
 				  struct task_struct *next)
 {
 	if ((read_ti_thread_flags(task_thread_info(prev)) &
 	     (_TIF_32BIT | _TIF_TSC_SIGSEGV)) !=
 	    (read_ti_thread_flags(task_thread_info(next)) &
 	     (_TIF_32BIT | _TIF_TSC_SIGSEGV)))
 		update_cntkctl_el1(next);
 }
 
 static int do_set_tsc_mode(unsigned int val)
 {
 	bool tsc_sigsegv;
 
 	if (val == PR_TSC_SIGSEGV)
 		tsc_sigsegv = true;
 	else if (val == PR_TSC_ENABLE)
 		tsc_sigsegv = false;
 	else
 		return -EINVAL;
 
 	preempt_disable();
 	update_thread_flag(TIF_TSC_SIGSEGV, tsc_sigsegv);
 	update_cntkctl_el1(current);
 	preempt_enable();
 
 	return 0;
 }
 
 static void permission_overlay_switch(struct task_struct *next)
 {
 	if (!system_supports_poe())
 		return;
 
 	current->thread.por_el0 = read_sysreg_s(SYS_POR_EL0);
 	if (current->thread.por_el0 != next->thread.por_el0) {
 		write_sysreg_s(next->thread.por_el0, SYS_POR_EL0);
 	}
 }
 
 /*
  * __switch_to() checks current->thread.sctlr_user as an optimisation. Therefore
  * this function must be called with preemption disabled and the update to
  * sctlr_user must be made in the same preemption disabled block so that
  * __switch_to() does not see the variable update before the SCTLR_EL1 one.
  */
 void update_sctlr_el1(u64 sctlr)
 {
 	/*
 	 * EnIA must not be cleared while in the kernel as this is necessary for
 	 * in-kernel PAC. It will be cleared on kernel exit if needed.
 	 */
 	sysreg_clear_set(sctlr_el1, SCTLR_USER_MASK & ~SCTLR_ELx_ENIA, sctlr);
 
 	/* ISB required for the kernel uaccess routines when setting TCF0. */
 	isb();
 }
 
 /*
  * Thread switching.
  */
 __notrace_funcgraph __sched
 struct task_struct *__switch_to(struct task_struct *prev,
 				struct task_struct *next)
 {
 	struct task_struct *last;
 
 	fpsimd_thread_switch(next);
 	tls_thread_switch(next);
 	hw_breakpoint_thread_switch(next);
 	contextidr_thread_switch(next);
 	entry_task_switch(next);
 	ssbs_thread_switch(next);
 	cntkctl_thread_switch(prev, next);
 	ptrauth_thread_switch_user(next);
 	permission_overlay_switch(next);
 	gcs_thread_switch(next);
+#ifdef CONFIG_ARM64_TSO
+	tso_thread_switch(next);
+#endif
 
 	/*
 	 * Complete any pending TLB or cache maintenance on this CPU in case
 	 * the thread migrates to a different CPU.
 	 * This full barrier is also required by the membarrier system
 	 * call.
 	 */
 	dsb(ish);
 
 	/*
 	 * MTE thread switching must happen after the DSB above to ensure that
 	 * any asynchronous tag check faults have been logged in the TFSR*_EL1
 	 * registers.
 	 */
 	mte_thread_switch(next);
 	/* avoid expensive SCTLR_EL1 accesses if no change */
 	if (prev->thread.sctlr_user != next->thread.sctlr_user)
 		update_sctlr_el1(next->thread.sctlr_user);
 
 	/* the actual thread switch */
 	last = cpu_switch_to(prev, next);
 
 	return last;
 }
 
 struct wchan_info {
 	unsigned long	pc;
 	int		count;
 };
 
 static bool get_wchan_cb(void *arg, unsigned long pc)
 {
 	struct wchan_info *wchan_info = arg;
 
 	if (!in_sched_functions(pc)) {
 		wchan_info->pc = pc;
 		return false;
 	}
 	return wchan_info->count++ < 16;
 }
 
 unsigned long __get_wchan(struct task_struct *p)
 {
 	struct wchan_info wchan_info = {
 		.pc = 0,
 		.count = 0,
 	};
 
 	if (!try_get_task_stack(p))
 		return 0;
 
 	arch_stack_walk(get_wchan_cb, &wchan_info, p, NULL);
 
 	put_task_stack(p);
 
 	return wchan_info.pc;
 }
 
 unsigned long arch_align_stack(unsigned long sp)
 {
 	if (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)
 		sp -= get_random_u32_below(PAGE_SIZE);
 	return sp & ~0xf;
 }
 
 #ifdef CONFIG_COMPAT
 int compat_elf_check_arch(const struct elf32_hdr *hdr)
 {
 	if (!system_supports_32bit_el0())
 		return false;
 
 	if ((hdr)->e_machine != EM_ARM)
 		return false;
 
 	if (!((hdr)->e_flags & EF_ARM_EABI_MASK))
 		return false;
 
 	/*
 	 * Prevent execve() of a 32-bit program from a deadline task
 	 * if the restricted affinity mask would be inadmissible on an
 	 * asymmetric system.
 	 */
 	return !static_branch_unlikely(&arm64_mismatched_32bit_el0) ||
 	       !dl_task_check_affinity(current, system_32bit_el0_cpumask());
 }
 #endif
 
 /*
  * Called from setup_new_exec() after (COMPAT_)SET_PERSONALITY.
  */
 void arch_setup_new_exec(void)
 {
 	unsigned long mmflags = 0;
 
 	if (is_compat_task()) {
 		mmflags = MMCF_AARCH32;
 
 		/*
 		 * Restrict the CPU affinity mask for a 32-bit task so that
 		 * it contains only 32-bit-capable CPUs.
 		 *
 		 * From the perspective of the task, this looks similar to
 		 * what would happen if the 64-bit-only CPUs were hot-unplugged
 		 * at the point of execve(), although we try a bit harder to
 		 * honour the cpuset hierarchy.
 		 */
 		if (static_branch_unlikely(&arm64_mismatched_32bit_el0))
 			force_compatible_cpus_allowed_ptr(current);
 	} else if (static_branch_unlikely(&arm64_mismatched_32bit_el0)) {
 		relax_compatible_cpus_allowed_ptr(current);
 	}
 
 	current->mm->context.flags = mmflags;
 	ptrauth_thread_init_user();
 	mte_thread_init_user();
 	do_set_tsc_mode(PR_TSC_ENABLE);
 
 	if (task_spec_ssb_noexec(current)) {
 		arch_prctl_spec_ctrl_set(current, PR_SPEC_STORE_BYPASS,
 					 PR_SPEC_ENABLE);
 	}
+
+#ifdef CONFIG_ARM64_TSO
+	modify_tso_enable(false);
+#endif
 }
 
 #ifdef CONFIG_ARM64_TAGGED_ADDR_ABI
 /*
  * Control the relaxed ABI allowing tagged user addresses into the kernel.
  */
 static unsigned int tagged_addr_disabled;
 
 long set_tagged_addr_ctrl(struct task_struct *task, unsigned long arg)
 {
 	unsigned long valid_mask = PR_TAGGED_ADDR_ENABLE;
 	struct thread_info *ti = task_thread_info(task);
 
 	if (is_compat_thread(ti))
 		return -EINVAL;
 
 	if (system_supports_mte())
 		valid_mask |= PR_MTE_TCF_SYNC | PR_MTE_TCF_ASYNC \
 			| PR_MTE_TAG_MASK;
 
 	if (arg & ~valid_mask)
 		return -EINVAL;
 
 	/*
 	 * Do not allow the enabling of the tagged address ABI if globally
 	 * disabled via sysctl abi.tagged_addr_disabled.
 	 */
 	if (arg & PR_TAGGED_ADDR_ENABLE && tagged_addr_disabled)
 		return -EINVAL;
 
 	if (set_mte_ctrl(task, arg) != 0)
 		return -EINVAL;
 
 	update_ti_thread_flag(ti, TIF_TAGGED_ADDR, arg & PR_TAGGED_ADDR_ENABLE);
 
 	return 0;
 }
 
 long get_tagged_addr_ctrl(struct task_struct *task)
 {
 	long ret = 0;
 	struct thread_info *ti = task_thread_info(task);
 
 	if (is_compat_thread(ti))
 		return -EINVAL;
 
 	if (test_ti_thread_flag(ti, TIF_TAGGED_ADDR))
 		ret = PR_TAGGED_ADDR_ENABLE;
 
 	ret |= get_mte_ctrl(task);
 
 	return ret;
 }
 
 /*
  * Global sysctl to disable the tagged user addresses support. This control
  * only prevents the tagged address ABI enabling via prctl() and does not
  * disable it for tasks that already opted in to the relaxed ABI.
  */
 
 static struct ctl_table tagged_addr_sysctl_table[] = {
 	{
 		.procname	= "tagged_addr_disabled",
 		.mode		= 0644,
 		.data		= &tagged_addr_disabled,
 		.maxlen		= sizeof(int),
 		.proc_handler	= proc_dointvec_minmax,
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= SYSCTL_ONE,
 	},
 };
 
 static int __init tagged_addr_init(void)
 {
 	if (!register_sysctl("abi", tagged_addr_sysctl_table))
 		return -EINVAL;
 	return 0;
 }
 
 core_initcall(tagged_addr_init);
 #endif	/* CONFIG_ARM64_TAGGED_ADDR_ABI */
 
 #ifdef CONFIG_BINFMT_ELF
 int arch_elf_adjust_prot(int prot, const struct arch_elf_state *state,
 			 bool has_interp, bool is_interp)
 {
 	/*
 	 * For dynamically linked executables the interpreter is
 	 * responsible for setting PROT_BTI on everything except
 	 * itself.
 	 */
 	if (is_interp != has_interp)
 		return prot;
 
 	if (!(state->flags & ARM64_ELF_BTI))
 		return prot;
 
 	if (prot & PROT_EXEC)
 		prot |= PROT_BTI;
 
 	return prot;
 }
 #endif
 
 int get_tsc_mode(unsigned long adr)
 {
 	unsigned int val;
 
 	if (is_compat_task())
 		return -EINVAL;
 
 	if (test_thread_flag(TIF_TSC_SIGSEGV))
 		val = PR_TSC_SIGSEGV;
 	else
 		val = PR_TSC_ENABLE;
 
 	return put_user(val, (unsigned int __user *)adr);
 }
 
 int set_tsc_mode(unsigned int val)
 {
 	if (is_compat_task())
 		return -EINVAL;
 
 	return do_set_tsc_mode(val);
 }
+
+static int arch_set_mem_model_default(struct task_struct *task)
+{
+	int return_error = 0;
+
+#ifdef CONFIG_ARM64_TSO
+	int modify_tso_enable_error = modify_tso_enable(false);
+
+	if (modify_tso_enable_error == -EOPNOTSUPP)
+		// TSO is the only other memory model on arm64.
+		// If TSO is not supported, then the default memory
+		// model must already be set.
+		return_error = 0;
+	else
+		return_error = modify_tso_enable_error;
+
+	if (!return_error)
+		task->thread.tso = false;
+
+	return return_error;
+#endif
+
+	return return_error;
+}
+
+#ifdef CONFIG_ARM64_TSO
+
+static int arch_set_mem_model_tso(struct task_struct *task)
+{
+	int error = modify_tso_enable(true);
+
+	if (!error)
+		task->thread.tso = true;
+
+	return error;
+}
+
+#endif /* CONFIG_ARM64_TSO */
+
+int arch_set_mem_model(struct task_struct *task, int memory_model)
+{
+	switch (memory_model) {
+	case PR_SET_MEM_MODEL_DEFAULT:
+		return arch_set_mem_model_default(task);
+#ifdef CONFIG_ARM64_TSO
+	case PR_SET_MEM_MODEL_TSO:
+		return arch_set_mem_model_tso(task);
+#endif /* CONFIG_ARM64_TSO */
+	default:
+		return -EINVAL;
+	}
+}
diff --git a/arch/arm64/kernel/tso.c b/arch/arm64/kernel/tso.c
new file mode 100644
index 000000000000..1b7795f52b36
--- /dev/null
+++ b/arch/arm64/kernel/tso.c
@@ -0,0 +1,93 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright © 2024 Apple Inc. All Rights Reserved.
+ * Disclaimer: IMPORTANT: This Apple software is supplied to you by Apple Inc.
+ * ("Apple") in consideration of your agreement to the following terms, and
+ * your use, installation, modification or redistribution of this Apple
+ * software constitutes acceptance of these terms. If you do not agree with
+ * these terms, please do not use, install, modify or redistribute this Apple
+ * software.
+ * In consideration of your agreement to abide by the following terms, and
+ * subject to these terms, Apple grants you a personal, non-exclusive license,
+ * under Apple's copyrights in this original Apple software (the "Apple
+ * Software"), to use, reproduce, modify and redistribute the Apple Software,
+ * with or without modifications, in source and/or binary forms; provided that
+ * if you redistribute the Apple Software in its entirety and without
+ * modifications, you must retain this notice and the following text and
+ * disclaimers in all such redistributions of the Apple Software. Neither the
+ * name, trademarks, service marks or logos of Apple Inc. may be used to
+ * endorse or promote products derived from the Apple Software without specific
+ * prior written permission from Apple. Except as expressly stated in this
+ * notice, no other rights or licenses, express or implied, are granted by
+ * Apple herein, including but not limited to any patent rights that may be
+ * infringed by your derivative works or by other works in which the Apple
+ * Software may be incorporated.
+ * The Apple Software is provided by Apple on an "AS IS" basis. APPLE MAKES NO
+ * WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED
+ * WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN
+ * COMBINATION WITH YOUR PRODUCTS.
+ * IN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION
+ * AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER
+ * THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR
+ * OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/sched.h>
+#include <linux/types.h>
+
+#include <asm/cputype.h>
+#include <asm/processor.h>
+#include <asm/sysreg.h>
+
+#ifdef CONFIG_ARM64_TSO
+
+static bool tso_supported(void)
+{
+	unsigned int cpuid_implementor = read_cpuid_implementor();
+	u64 aidr = read_sysreg(aidr_el1);
+
+	return (cpuid_implementor == ARM_CPU_IMP_APPLE) &&
+		(aidr & SYS_AIDR_EL1_TSO_MASK);
+}
+
+static int tso_enabled(void)
+{
+	if (!tso_supported())
+		return -EOPNOTSUPP;
+
+	u64 actlr_el1 = read_sysreg(actlr_el1);
+
+	return !!(actlr_el1 & SYS_ACTLR_EL1_TSOEN_MASK);
+}
+
+int modify_tso_enable(bool tso_enable)
+{
+	if (!tso_supported())
+		return -EOPNOTSUPP;
+
+	u64 actlr_el1_old = read_sysreg(actlr_el1);
+	u64 actlr_el1_new =
+		(actlr_el1_old & ~SYS_ACTLR_EL1_TSOEN_MASK) |
+		(tso_enable << SYS_ACTLR_EL1_TSOEN_SHIFT);
+
+	write_sysreg(actlr_el1_new, actlr_el1);
+
+	if (tso_enabled() != tso_enable)
+		return -EOPNOTSUPP;
+
+	return 0;
+}
+
+void tso_thread_switch(struct task_struct *next)
+{
+	if (tso_supported()) {
+		current->thread.tso = tso_enabled();
+		modify_tso_enable(next->thread.tso);
+	}
+}
+
+#endif /* CONFIG_ARM64_TSO */
diff --git a/include/uapi/linux/prctl.h b/include/uapi/linux/prctl.h
index 5c6080680cb2..fea14e217548 100644
--- a/include/uapi/linux/prctl.h
+++ b/include/uapi/linux/prctl.h
@@ -1,101 +1,102 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+
 #ifndef _LINUX_PRCTL_H
 #define _LINUX_PRCTL_H
 
 #include <linux/types.h>
 
 /* Values to pass as first argument to prctl() */
 
 #define PR_SET_PDEATHSIG  1  /* Second arg is a signal */
 #define PR_GET_PDEATHSIG  2  /* Second arg is a ptr to return the signal */
 
 /* Get/set current->mm->dumpable */
 #define PR_GET_DUMPABLE   3
 #define PR_SET_DUMPABLE   4
 
 /* Get/set unaligned access control bits (if meaningful) */
 #define PR_GET_UNALIGN	  5
 #define PR_SET_UNALIGN	  6
 # define PR_UNALIGN_NOPRINT	1	/* silently fix up unaligned user accesses */
 # define PR_UNALIGN_SIGBUS	2	/* generate SIGBUS on unaligned user access */
 
 /* Get/set whether or not to drop capabilities on setuid() away from
  * uid 0 (as per security/commoncap.c) */
 #define PR_GET_KEEPCAPS   7
 #define PR_SET_KEEPCAPS   8
 
 /* Get/set floating-point emulation control bits (if meaningful) */
 #define PR_GET_FPEMU  9
 #define PR_SET_FPEMU 10
 # define PR_FPEMU_NOPRINT	1	/* silently emulate fp operations accesses */
 # define PR_FPEMU_SIGFPE	2	/* don't emulate fp operations, send SIGFPE instead */
 
 /* Get/set floating-point exception mode (if meaningful) */
 #define PR_GET_FPEXC	11
 #define PR_SET_FPEXC	12
 # define PR_FP_EXC_SW_ENABLE	0x80	/* Use FPEXC for FP exception enables */
 # define PR_FP_EXC_DIV		0x010000	/* floating point divide by zero */
 # define PR_FP_EXC_OVF		0x020000	/* floating point overflow */
 # define PR_FP_EXC_UND		0x040000	/* floating point underflow */
 # define PR_FP_EXC_RES		0x080000	/* floating point inexact result */
 # define PR_FP_EXC_INV		0x100000	/* floating point invalid operation */
 # define PR_FP_EXC_DISABLED	0	/* FP exceptions disabled */
 # define PR_FP_EXC_NONRECOV	1	/* async non-recoverable exc. mode */
 # define PR_FP_EXC_ASYNC	2	/* async recoverable exception mode */
 # define PR_FP_EXC_PRECISE	3	/* precise exception mode */
 
 /* Get/set whether we use statistical process timing or accurate timestamp
  * based process timing */
 #define PR_GET_TIMING   13
 #define PR_SET_TIMING   14
 # define PR_TIMING_STATISTICAL  0       /* Normal, traditional,
                                                    statistical process timing */
 # define PR_TIMING_TIMESTAMP    1       /* Accurate timestamp based
                                                    process timing */
 
 #define PR_SET_NAME    15		/* Set process name */
 #define PR_GET_NAME    16		/* Get process name */
 
 /* Get/set process endian */
 #define PR_GET_ENDIAN	19
 #define PR_SET_ENDIAN	20
 # define PR_ENDIAN_BIG		0
 # define PR_ENDIAN_LITTLE	1	/* True little endian mode */
 # define PR_ENDIAN_PPC_LITTLE	2	/* "PowerPC" pseudo little endian */
 
 /* Get/set process seccomp mode */
 #define PR_GET_SECCOMP	21
 #define PR_SET_SECCOMP	22
 
 /* Get/set the capability bounding set (as per security/commoncap.c) */
 #define PR_CAPBSET_READ 23
 #define PR_CAPBSET_DROP 24
 
 /* Get/set the process' ability to use the timestamp counter instruction */
 #define PR_GET_TSC 25
 #define PR_SET_TSC 26
 # define PR_TSC_ENABLE		1	/* allow the use of the timestamp counter */
 # define PR_TSC_SIGSEGV		2	/* throw a SIGSEGV instead of reading the TSC */
 
 /* Get/set securebits (as per security/commoncap.c) */
 #define PR_GET_SECUREBITS 27
 #define PR_SET_SECUREBITS 28
 
 /*
  * Get/set the timerslack as used by poll/select/nanosleep
  * A value of 0 means "use default"
  */
 #define PR_SET_TIMERSLACK 29
 #define PR_GET_TIMERSLACK 30
 
 #define PR_TASK_PERF_EVENTS_DISABLE		31
 #define PR_TASK_PERF_EVENTS_ENABLE		32
 
 /*
  * Set early/late kill mode for hwpoison memory corruption.
  * This influences when the process gets killed on a memory corruption.
  */
 #define PR_MCE_KILL	33
 # define PR_MCE_KILL_CLEAR   0
 # define PR_MCE_KILL_SET     1
 
@@ -256,101 +257,142 @@ struct prctl_mm_map {
 #define PR_SET_SYSCALL_USER_DISPATCH	59
 # define PR_SYS_DISPATCH_OFF		0
 # define PR_SYS_DISPATCH_ON		1
 /* The control values for the user space selector when dispatch is enabled */
 # define SYSCALL_DISPATCH_FILTER_ALLOW	0
 # define SYSCALL_DISPATCH_FILTER_BLOCK	1
 
 /* Set/get enabled arm64 pointer authentication keys */
 #define PR_PAC_SET_ENABLED_KEYS		60
 #define PR_PAC_GET_ENABLED_KEYS		61
 
 /* Request the scheduler to share a core */
 #define PR_SCHED_CORE			62
 # define PR_SCHED_CORE_GET		0
 # define PR_SCHED_CORE_CREATE		1 /* create unique core_sched cookie */
 # define PR_SCHED_CORE_SHARE_TO		2 /* push core_sched cookie to pid */
 # define PR_SCHED_CORE_SHARE_FROM	3 /* pull core_sched cookie to pid */
 # define PR_SCHED_CORE_MAX		4
 # define PR_SCHED_CORE_SCOPE_THREAD		0
 # define PR_SCHED_CORE_SCOPE_THREAD_GROUP	1
 # define PR_SCHED_CORE_SCOPE_PROCESS_GROUP	2
 
 /* arm64 Scalable Matrix Extension controls */
 /* Flag values must be in sync with SVE versions */
 #define PR_SME_SET_VL			63	/* set task vector length */
 # define PR_SME_SET_VL_ONEXEC		(1 << 18) /* defer effect until exec */
 #define PR_SME_GET_VL			64	/* get task vector length */
 /* Bits common to PR_SME_SET_VL and PR_SME_GET_VL */
 # define PR_SME_VL_LEN_MASK		0xffff
 # define PR_SME_VL_INHERIT		(1 << 17) /* inherit across exec */
 
 /* Memory deny write / execute */
 #define PR_SET_MDWE			65
 # define PR_MDWE_REFUSE_EXEC_GAIN	(1UL << 0)
 # define PR_MDWE_NO_INHERIT		(1UL << 1)
 
 #define PR_GET_MDWE			66
 
 #define PR_SET_VMA		0x53564d41
 # define PR_SET_VMA_ANON_NAME		0
 
 #define PR_GET_AUXV			0x41555856
 
 #define PR_SET_MEMORY_MERGE		67
 #define PR_GET_MEMORY_MERGE		68
 
 #define PR_RISCV_V_SET_CONTROL		69
 #define PR_RISCV_V_GET_CONTROL		70
 # define PR_RISCV_V_VSTATE_CTRL_DEFAULT		0
 # define PR_RISCV_V_VSTATE_CTRL_OFF		1
 # define PR_RISCV_V_VSTATE_CTRL_ON		2
 # define PR_RISCV_V_VSTATE_CTRL_INHERIT		(1 << 4)
 # define PR_RISCV_V_VSTATE_CTRL_CUR_MASK	0x3
 # define PR_RISCV_V_VSTATE_CTRL_NEXT_MASK	0xc
 # define PR_RISCV_V_VSTATE_CTRL_MASK		0x1f
 
 #define PR_RISCV_SET_ICACHE_FLUSH_CTX	71
 # define PR_RISCV_CTX_SW_FENCEI_ON	0
 # define PR_RISCV_CTX_SW_FENCEI_OFF	1
 # define PR_RISCV_SCOPE_PER_PROCESS	0
 # define PR_RISCV_SCOPE_PER_THREAD	1
 
 /* PowerPC Dynamic Execution Control Register (DEXCR) controls */
 #define PR_PPC_GET_DEXCR		72
 #define PR_PPC_SET_DEXCR		73
 /* DEXCR aspect to act on */
 # define PR_PPC_DEXCR_SBHE		0 /* Speculative branch hint enable */
 # define PR_PPC_DEXCR_IBRTPD		1 /* Indirect branch recurrent target prediction disable */
 # define PR_PPC_DEXCR_SRAPD		2 /* Subroutine return address prediction disable */
 # define PR_PPC_DEXCR_NPHIE		3 /* Non-privileged hash instruction enable */
 /* Action to apply / return */
 # define PR_PPC_DEXCR_CTRL_EDITABLE	 0x1 /* Aspect can be modified with PR_PPC_SET_DEXCR */
 # define PR_PPC_DEXCR_CTRL_SET		 0x2 /* Set the aspect for this process */
 # define PR_PPC_DEXCR_CTRL_CLEAR	 0x4 /* Clear the aspect for this process */
 # define PR_PPC_DEXCR_CTRL_SET_ONEXEC	 0x8 /* Set the aspect on exec */
 # define PR_PPC_DEXCR_CTRL_CLEAR_ONEXEC	0x10 /* Clear the aspect on exec */
 # define PR_PPC_DEXCR_CTRL_MASK		0x1f
 
 /*
  * Get the current shadow stack configuration for the current thread,
  * this will be the value configured via PR_SET_SHADOW_STACK_STATUS.
  */
 #define PR_GET_SHADOW_STACK_STATUS      74
 
 /*
  * Set the current shadow stack configuration.  Enabling the shadow
  * stack will cause a shadow stack to be allocated for the thread.
  */
 #define PR_SET_SHADOW_STACK_STATUS      75
 # define PR_SHADOW_STACK_ENABLE         (1UL << 0)
 # define PR_SHADOW_STACK_WRITE		(1UL << 1)
 # define PR_SHADOW_STACK_PUSH		(1UL << 2)
 
 /*
  * Prevent further changes to the specified shadow stack
  * configuration.  All bits may be locked via this call, including
  * undefined bits.
  */
 #define PR_LOCK_SHADOW_STACK_STATUS      76
 
+/*
+ * Copyright © 2024 Apple Inc. All Rights Reserved.
+ * Disclaimer: IMPORTANT: This Apple software is supplied to you by Apple Inc.
+ * ("Apple") in consideration of your agreement to the following terms, and
+ * your use, installation, modification or redistribution of this Apple
+ * software constitutes acceptance of these terms. If you do not agree with
+ * these terms, please do not use, install, modify or redistribute this Apple
+ * software.
+ * In consideration of your agreement to abide by the following terms, and
+ * subject to these terms, Apple grants you a personal, non-exclusive license,
+ * under Apple's copyrights in this original Apple software (the "Apple
+ * Software"), to use, reproduce, modify and redistribute the Apple Software,
+ * with or without modifications, in source and/or binary forms; provided that
+ * if you redistribute the Apple Software in its entirety and without
+ * modifications, you must retain this notice and the following text and
+ * disclaimers in all such redistributions of the Apple Software. Neither the
+ * name, trademarks, service marks or logos of Apple Inc. may be used to
+ * endorse or promote products derived from the Apple Software without specific
+ * prior written permission from Apple. Except as expressly stated in this
+ * notice, no other rights or licenses, express or implied, are granted by
+ * Apple herein, including but not limited to any patent rights that may be
+ * infringed by your derivative works or by other works in which the Apple
+ * Software may be incorporated.
+ * The Apple Software is provided by Apple on an "AS IS" basis. APPLE MAKES NO
+ * WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED
+ * WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN
+ * COMBINATION WITH YOUR PRODUCTS.
+ * IN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION
+ * AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER
+ * THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR
+ * OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+/* Set the CPU memory model */
+#define PR_SET_MEM_MODEL		0x4d4d444c
+# define PR_SET_MEM_MODEL_DEFAULT	0
+# define PR_SET_MEM_MODEL_TSO		1
+
 #endif /* _LINUX_PRCTL_H */
diff --git a/kernel/sys.c b/kernel/sys.c
index c4c701c6f0b4..e523a088d792 100644
--- a/kernel/sys.c
+++ b/kernel/sys.c
@@ -2237,200 +2237,205 @@ static int prctl_set_mm(int opt, unsigned long addr,
 	}
 
 	error = validate_prctl_map_addr(&prctl_map);
 	if (error)
 		goto out;
 
 	switch (opt) {
 	/*
 	 * If command line arguments and environment
 	 * are placed somewhere else on stack, we can
 	 * set them up here, ARG_START/END to setup
 	 * command line arguments and ENV_START/END
 	 * for environment.
 	 */
 	case PR_SET_MM_START_STACK:
 	case PR_SET_MM_ARG_START:
 	case PR_SET_MM_ARG_END:
 	case PR_SET_MM_ENV_START:
 	case PR_SET_MM_ENV_END:
 		if (!vma) {
 			error = -EFAULT;
 			goto out;
 		}
 	}
 
 	mm->start_code	= prctl_map.start_code;
 	mm->end_code	= prctl_map.end_code;
 	mm->start_data	= prctl_map.start_data;
 	mm->end_data	= prctl_map.end_data;
 	mm->start_brk	= prctl_map.start_brk;
 	mm->brk		= prctl_map.brk;
 	mm->start_stack	= prctl_map.start_stack;
 	mm->arg_start	= prctl_map.arg_start;
 	mm->arg_end	= prctl_map.arg_end;
 	mm->env_start	= prctl_map.env_start;
 	mm->env_end	= prctl_map.env_end;
 
 	error = 0;
 out:
 	spin_unlock(&mm->arg_lock);
 	mmap_read_unlock(mm);
 	return error;
 }
 
 #ifdef CONFIG_CHECKPOINT_RESTORE
 static int prctl_get_tid_address(struct task_struct *me, int __user * __user *tid_addr)
 {
 	return put_user(me->clear_child_tid, tid_addr);
 }
 #else
 static int prctl_get_tid_address(struct task_struct *me, int __user * __user *tid_addr)
 {
 	return -EINVAL;
 }
 #endif
 
 static int propagate_has_child_subreaper(struct task_struct *p, void *data)
 {
 	/*
 	 * If task has has_child_subreaper - all its descendants
 	 * already have these flag too and new descendants will
 	 * inherit it on fork, skip them.
 	 *
 	 * If we've found child_reaper - skip descendants in
 	 * it's subtree as they will never get out pidns.
 	 */
 	if (p->signal->has_child_subreaper ||
 	    is_child_reaper(task_pid(p)))
 		return 0;
 
 	p->signal->has_child_subreaper = 1;
 	return 1;
 }
 
 int __weak arch_prctl_spec_ctrl_get(struct task_struct *t, unsigned long which)
 {
 	return -EINVAL;
 }
 
 int __weak arch_prctl_spec_ctrl_set(struct task_struct *t, unsigned long which,
 				    unsigned long ctrl)
 {
 	return -EINVAL;
 }
 
 int __weak arch_get_shadow_stack_status(struct task_struct *t, unsigned long __user *status)
 {
 	return -EINVAL;
 }
 
 int __weak arch_set_shadow_stack_status(struct task_struct *t, unsigned long status)
 {
 	return -EINVAL;
 }
 
 int __weak arch_lock_shadow_stack_status(struct task_struct *t, unsigned long status)
 {
 	return -EINVAL;
 }
 
+int __weak arch_set_mem_model(struct task_struct *task, int memory_model)
+{
+	return -EINVAL;
+}
+
 #define PR_IO_FLUSHER (PF_MEMALLOC_NOIO | PF_LOCAL_THROTTLE)
 
 #ifdef CONFIG_ANON_VMA_NAME
 
 #define ANON_VMA_NAME_MAX_LEN		80
 #define ANON_VMA_NAME_INVALID_CHARS	"\\`$[]"
 
 static inline bool is_valid_name_char(char ch)
 {
 	/* printable ascii characters, excluding ANON_VMA_NAME_INVALID_CHARS */
 	return ch > 0x1f && ch < 0x7f &&
 		!strchr(ANON_VMA_NAME_INVALID_CHARS, ch);
 }
 
 static int prctl_set_vma(unsigned long opt, unsigned long addr,
 			 unsigned long size, unsigned long arg)
 {
 	struct mm_struct *mm = current->mm;
 	const char __user *uname;
 	struct anon_vma_name *anon_name = NULL;
 	int error;
 
 	switch (opt) {
 	case PR_SET_VMA_ANON_NAME:
 		uname = (const char __user *)arg;
 		if (uname) {
 			char *name, *pch;
 
 			name = strndup_user(uname, ANON_VMA_NAME_MAX_LEN);
 			if (IS_ERR(name))
 				return PTR_ERR(name);
 
 			for (pch = name; *pch != '\0'; pch++) {
 				if (!is_valid_name_char(*pch)) {
 					kfree(name);
 					return -EINVAL;
 				}
 			}
 			/* anon_vma has its own copy */
 			anon_name = anon_vma_name_alloc(name);
 			kfree(name);
 			if (!anon_name)
 				return -ENOMEM;
 
 		}
 
 		mmap_write_lock(mm);
 		error = madvise_set_anon_name(mm, addr, size, anon_name);
 		mmap_write_unlock(mm);
 		anon_vma_name_put(anon_name);
 		break;
 	default:
 		error = -EINVAL;
 	}
 
 	return error;
 }
 
 #else /* CONFIG_ANON_VMA_NAME */
 static int prctl_set_vma(unsigned long opt, unsigned long start,
 			 unsigned long size, unsigned long arg)
 {
 	return -EINVAL;
 }
 #endif /* CONFIG_ANON_VMA_NAME */
 
 static inline unsigned long get_current_mdwe(void)
 {
 	unsigned long ret = 0;
 
 	if (test_bit(MMF_HAS_MDWE, &current->mm->flags))
 		ret |= PR_MDWE_REFUSE_EXEC_GAIN;
 	if (test_bit(MMF_HAS_MDWE_NO_INHERIT, &current->mm->flags))
 		ret |= PR_MDWE_NO_INHERIT;
 
 	return ret;
 }
 
 static inline int prctl_set_mdwe(unsigned long bits, unsigned long arg3,
 				 unsigned long arg4, unsigned long arg5)
 {
 	unsigned long current_bits;
 
 	if (arg3 || arg4 || arg5)
 		return -EINVAL;
 
 	if (bits & ~(PR_MDWE_REFUSE_EXEC_GAIN | PR_MDWE_NO_INHERIT))
 		return -EINVAL;
 
 	/* NO_INHERIT only makes sense with REFUSE_EXEC_GAIN */
 	if (bits & PR_MDWE_NO_INHERIT && !(bits & PR_MDWE_REFUSE_EXEC_GAIN))
 		return -EINVAL;
 
 	/*
 	 * EOPNOTSUPP might be more appropriate here in principle, but
 	 * existing userspace depends on EINVAL specifically.
 	 */
 	if (!arch_memory_deny_write_exec_supported())
 		return -EINVAL;
 
@@ -2712,200 +2717,205 @@ SYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,
 	case PR_SET_IO_FLUSHER:
 		if (!capable(CAP_SYS_RESOURCE))
 			return -EPERM;
 
 		if (arg3 || arg4 || arg5)
 			return -EINVAL;
 
 		if (arg2 == 1)
 			current->flags |= PR_IO_FLUSHER;
 		else if (!arg2)
 			current->flags &= ~PR_IO_FLUSHER;
 		else
 			return -EINVAL;
 		break;
 	case PR_GET_IO_FLUSHER:
 		if (!capable(CAP_SYS_RESOURCE))
 			return -EPERM;
 
 		if (arg2 || arg3 || arg4 || arg5)
 			return -EINVAL;
 
 		error = (current->flags & PR_IO_FLUSHER) == PR_IO_FLUSHER;
 		break;
 	case PR_SET_SYSCALL_USER_DISPATCH:
 		error = set_syscall_user_dispatch(arg2, arg3, arg4,
 						  (char __user *) arg5);
 		break;
 #ifdef CONFIG_SCHED_CORE
 	case PR_SCHED_CORE:
 		error = sched_core_share_pid(arg2, arg3, arg4, arg5);
 		break;
 #endif
 	case PR_SET_MDWE:
 		error = prctl_set_mdwe(arg2, arg3, arg4, arg5);
 		break;
 	case PR_GET_MDWE:
 		error = prctl_get_mdwe(arg2, arg3, arg4, arg5);
 		break;
 	case PR_PPC_GET_DEXCR:
 		if (arg3 || arg4 || arg5)
 			return -EINVAL;
 		error = PPC_GET_DEXCR_ASPECT(me, arg2);
 		break;
 	case PR_PPC_SET_DEXCR:
 		if (arg4 || arg5)
 			return -EINVAL;
 		error = PPC_SET_DEXCR_ASPECT(me, arg2, arg3);
 		break;
 	case PR_SET_VMA:
 		error = prctl_set_vma(arg2, arg3, arg4, arg5);
 		break;
 	case PR_GET_AUXV:
 		if (arg4 || arg5)
 			return -EINVAL;
 		error = prctl_get_auxv((void __user *)arg2, arg3);
 		break;
 #ifdef CONFIG_KSM
 	case PR_SET_MEMORY_MERGE:
 		if (arg3 || arg4 || arg5)
 			return -EINVAL;
 		if (mmap_write_lock_killable(me->mm))
 			return -EINTR;
 
 		if (arg2)
 			error = ksm_enable_merge_any(me->mm);
 		else
 			error = ksm_disable_merge_any(me->mm);
 		mmap_write_unlock(me->mm);
 		break;
 	case PR_GET_MEMORY_MERGE:
 		if (arg2 || arg3 || arg4 || arg5)
 			return -EINVAL;
 
 		error = !!test_bit(MMF_VM_MERGE_ANY, &me->mm->flags);
 		break;
 #endif
 	case PR_RISCV_V_SET_CONTROL:
 		error = RISCV_V_SET_CONTROL(arg2);
 		break;
 	case PR_RISCV_V_GET_CONTROL:
 		error = RISCV_V_GET_CONTROL();
 		break;
 	case PR_RISCV_SET_ICACHE_FLUSH_CTX:
 		error = RISCV_SET_ICACHE_FLUSH_CTX(arg2, arg3);
 		break;
 	case PR_GET_SHADOW_STACK_STATUS:
 		if (arg3 || arg4 || arg5)
 			return -EINVAL;
 		error = arch_get_shadow_stack_status(me, (unsigned long __user *) arg2);
 		break;
 	case PR_SET_SHADOW_STACK_STATUS:
 		if (arg3 || arg4 || arg5)
 			return -EINVAL;
 		error = arch_set_shadow_stack_status(me, arg2);
 		break;
 	case PR_LOCK_SHADOW_STACK_STATUS:
 		if (arg3 || arg4 || arg5)
 			return -EINVAL;
 		error = arch_lock_shadow_stack_status(me, arg2);
 		break;
+	case PR_SET_MEM_MODEL:
+		if (arg3 || arg4 || arg5)
+			return -EINVAL;
+		error = arch_set_mem_model(current, arg2);
+		break;
 	default:
 		error = -EINVAL;
 		break;
 	}
 	return error;
 }
 
 SYSCALL_DEFINE3(getcpu, unsigned __user *, cpup, unsigned __user *, nodep,
 		struct getcpu_cache __user *, unused)
 {
 	int err = 0;
 	int cpu = raw_smp_processor_id();
 
 	if (cpup)
 		err |= put_user(cpu, cpup);
 	if (nodep)
 		err |= put_user(cpu_to_node(cpu), nodep);
 	return err ? -EFAULT : 0;
 }
 
 /**
  * do_sysinfo - fill in sysinfo struct
  * @info: pointer to buffer to fill
  */
 static int do_sysinfo(struct sysinfo *info)
 {
 	unsigned long mem_total, sav_total;
 	unsigned int mem_unit, bitcount;
 	struct timespec64 tp;
 
 	memset(info, 0, sizeof(struct sysinfo));
 
 	ktime_get_boottime_ts64(&tp);
 	timens_add_boottime(&tp);
 	info->uptime = tp.tv_sec + (tp.tv_nsec ? 1 : 0);
 
 	get_avenrun(info->loads, 0, SI_LOAD_SHIFT - FSHIFT);
 
 	info->procs = nr_threads;
 
 	si_meminfo(info);
 	si_swapinfo(info);
 
 	/*
 	 * If the sum of all the available memory (i.e. ram + swap)
 	 * is less than can be stored in a 32 bit unsigned long then
 	 * we can be binary compatible with 2.2.x kernels.  If not,
 	 * well, in that case 2.2.x was broken anyways...
 	 *
 	 *  -Erik Andersen <andersee@debian.org>
 	 */
 
 	mem_total = info->totalram + info->totalswap;
 	if (mem_total < info->totalram || mem_total < info->totalswap)
 		goto out;
 	bitcount = 0;
 	mem_unit = info->mem_unit;
 	while (mem_unit > 1) {
 		bitcount++;
 		mem_unit >>= 1;
 		sav_total = mem_total;
 		mem_total <<= 1;
 		if (mem_total < sav_total)
 			goto out;
 	}
 
 	/*
 	 * If mem_total did not overflow, multiply all memory values by
 	 * info->mem_unit and set it to 1.  This leaves things compatible
 	 * with 2.2.x, and also retains compatibility with earlier 2.4.x
 	 * kernels...
 	 */
 
 	info->mem_unit = 1;
 	info->totalram <<= bitcount;
 	info->freeram <<= bitcount;
 	info->sharedram <<= bitcount;
 	info->bufferram <<= bitcount;
 	info->totalswap <<= bitcount;
 	info->freeswap <<= bitcount;
 	info->totalhigh <<= bitcount;
 	info->freehigh <<= bitcount;
 
 out:
 	return 0;
 }
 
 SYSCALL_DEFINE1(sysinfo, struct sysinfo __user *, info)
 {
 	struct sysinfo val;
 
 	do_sysinfo(&val);
 
 	if (copy_to_user(info, &val, sizeof(struct sysinfo)))
 		return -EFAULT;
 
 	return 0;
 }
 
 #ifdef CONFIG_COMPAT
